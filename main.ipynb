{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projekt zur Spurerkennung im Wahlfach \"Digitale Bildverarbeitung\" \n",
    "\n",
    "In diesem Jupyter-Notebook wird eine Fahrbahnmarkierungserkennung implementiert. Anhand von Bildern und Videos von Udacity und KITTI wird diese Erkennung auf verschiedenste Weise getestet.\n",
    "\n",
    "Um eine Fahrbahn erkennen zu können müssen folgende Schritte während des Programmablaufs abgearbeitet werden:\n",
    "\n",
    "    - Kamerakalibrierung\n",
    "    - Perspektivtransformation\n",
    "    - Maskieren des Bildes mit gelben und weißen Farbmasken\n",
    "    - \"Sliding Windows\"\n",
    "    - Berechnung der Polynome für die Eingrenzung der Fahrbahn\n",
    "    - Rücktransformation der berechneten Punkte der Polynome\n",
    "    - Einzeichnen der Fläche zwischen den Polynomen\n",
    "\n",
    "Im folgenden werden die jeweils verwendeten Funktionen genauer erklärt, am Ende finden sich die Bilder zur Kamerakalibrierung sowie zur Spurerkennung.\n",
    "\n",
    "<sup>Bearbeitet wurde das Projekt von Alex Schulte(), Eddi () und Marcel Fleck (9611872)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Measurement\n",
    "\n",
    "Diese Funktionen dienen der Bestimmung von zeitaufwändigen Arbeitsschritten, um eine Verbesserung der Framerate zu erleichtern.\n",
    "\n",
    "In einem Array wird ein Objekt hinterlegt, in dem der Name, die Start- und Endzeit sowie die Anzahl an Aufrufen hinterlegt wird (siehe ```start_time_measurement``` und ```end_time_measurement```).\n",
    "\n",
    "Diese Informationen werden dann in einer Tabelle nach Beendigung des Programmdurchlaufs visualisiert (siehe ```analyse_time_measurement```).\n",
    "\n",
    "Abbildung 1 zeigt ein Beispile für eine solche Tabelle:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/documentation/timemeasurement.png\" height=\"35%\" width=\"35%\" />\n",
    "  <br>\n",
    "  <em>Abbildung 1: Tabelle zur Darstellung der Zeitmessungsergebnisse</em>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_measurements = {}\n",
    "\n",
    "def start_time_measurement(eventName):\n",
    "    add = False\n",
    "    if eventName not in time_measurements:\n",
    "        time_measurements[eventName] = {\n",
    "            \"name\": eventName,\n",
    "            \"start\": [],\n",
    "            \"end\": [],\n",
    "            \"count\": 0\n",
    "        }\n",
    "        add = True\n",
    "    elif len(time_measurements[eventName][\"start\"]) > len(time_measurements[eventName][\"end\"]):\n",
    "        print(f\"Time measure error: Event '{eventName}' not finished before reassignment!\")\n",
    "    else:\n",
    "       add = True\n",
    "    \n",
    "    #start measurement as late as possible\n",
    "    if add:\n",
    "        time_measurements[eventName][\"start\"].append(time.perf_counter_ns())\n",
    "\n",
    "def end_time_measurement(eventName):\n",
    "    #end measurement as fast as possible\n",
    "    temp_time = time.perf_counter_ns()\n",
    "    if eventName not in time_measurements:\n",
    "        print(f\"Time measure error: Event '{eventName}' not defined!\")\n",
    "    elif len(time_measurements[eventName][\"end\"]) >= len(time_measurements[eventName][\"start\"]):\n",
    "        print(f\"Time measure error: Event '{eventName}' not started before reassignment!\")\n",
    "    else:\n",
    "        time_measurements[eventName][\"end\"].append(temp_time)\n",
    "        time_measurements[eventName][\"count\"] += 1\n",
    "\n",
    "def analyse_time_measurements():\n",
    "    time_measurements_table = PrettyTable([\"Name\", \"Avg. [ms]\", \"Min. [ms]\", \"Max. [ms]\", \"Occurrences [compl.]\"])\n",
    "    time_measurements_table.align[\"Name\"] = \"l\"\n",
    "    time_measurements_table.align[\"Avg. [ms]\"] = \"r\"\n",
    "    time_measurements_table.align[\"Min. [ms]\"] = \"r\"\n",
    "    time_measurements_table.align[\"Max. [ms]\"] = \"r\"\n",
    "    time_measurements_table.align[\"Occurrences [compl.]\"] = \"r\"\n",
    "    for key, event in time_measurements.items():\n",
    "        timings = []\n",
    "        if len(event[\"start\"]) != len(event[\"end\"]):\n",
    "            print(f\"Time measure error: Event '{key}' has different amounts of values for start and end times!\")\n",
    "        else:\n",
    "            #exclude 0 values\n",
    "            for i in range(len(event[\"start\"])):\n",
    "                timing = (event[\"end\"][i] - event[\"start\"][i])\n",
    "                if timing >= 0:\n",
    "                    timing = timing / (1000 * 1000) #convert from ns to ms\n",
    "                    timings.append(timing)\n",
    "\n",
    "            event[\"min\"] = min(timings) \n",
    "            event[\"max\"] = max(timings)\n",
    "            event[\"avg\"] = sum(timings) / len(event[\"start\"])\n",
    "\n",
    "            time_measurements_table.add_row([key, '{0:.2f}'.format(event[\"avg\"]), '{0:.2f}'.format(event[\"min\"]), '{0:.2f}'.format(event[\"max\"]), event[\"count\"]])\n",
    "    print(time_measurements_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kamerakalibrierung\n",
    "\n",
    "Die Abbildung eines 3D-Objekts auf eine 2D-Bildebene mit einer Kamera wird durch die internen Größen einer Kamera beeinflusst. Dazu gehören zum Beispiel die Bildmitte, die Brennweite und auch Kameraverzerrungsparameter.\n",
    "\n",
    "Mit Hilfe der Kamerakalibrierung werden die sogenannten \"intrinsische\" und \"extrinsische\" Parameter berechnet. Intrinsische Parameter beschreiben die Kalibriermatrix (Kamera zu Pixel), während die extrinsischen Parameter die Beziehung zwischen dem Koordinatensystem der Kamera und dem Welt-Koordinatensystem beschreiben (Welt zu Kamera).\n",
    "\n",
    "Zusammen bilden diese Parameter eine Projektionsmatrix, die es ermöglicht, gemachte Bilder im Welt-Koordinatensystem darzustellen (die Bilder werden entzerrt). \n",
    "\n",
    "In folgendem Abschnitt wird die Kamera mit Hilfe von Schachbrett-Bildern kalibriert. Diese Aufgabe mit den Ergebnissen auf den Schachbrettbildern befinden sich im JupyterNotebook [Projekt_Spurerkennung_v2.jpynb](Projekt_Spurerkennung_v2.ipynb).\n",
    "\n",
    "\n",
    "Mit den unten definierten Funktionen (siehe ```undistort_image``` und ```undistort_image_remap```) werden dann die Frames der Videos anhand der berechneten Transformations-Matrix entzerrt, um eine tatsächliche Darstellung im Welt-Koordinatensystem zu erhalten. Eine Implementierung von zwei Funktionen lässt mit dem Erreichen von einer ungefähr 50% schnelleren Verarbeitung begründen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of chessboard, minimum error with (7, 6), but there were severe artefacts at the borders (see error calculation at the end)\n",
    "x, y = 9, 6\n",
    "\n",
    "# termination criteria\n",
    "criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((y * x, 3), np.float32)\n",
    "objp[:, :2] = np.mgrid[0:x, 0:y].T.reshape(-1, 2)\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = []  # 3d point in real world space\n",
    "imgpoints = []  # 2d points in image plane.\n",
    "\n",
    "\n",
    "images = glob.glob(\"./img/Udacity/calib/*.jpg\")\n",
    "for i, fname in enumerate(images):\n",
    "    img = cv.imread(fname)\n",
    "    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find the chess board corners\n",
    "    ret, corners = cv.findChessboardCorners(gray, (x, y), None)\n",
    "    # If found, add object points, image points (after refining them)\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "        corners2 = cv.cornerSubPix(\n",
    "            gray, corners, (11, 11), (-1, -1), criteria\n",
    "        )  # improve accuracy of corners\n",
    "        imgpoints.append(corners)\n",
    "\n",
    "ret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(\n",
    "    objpoints, imgpoints, gray.shape[::-1], None, None\n",
    ")\n",
    "\n",
    "\n",
    "h, w = img.shape[:2]\n",
    "newcameramtx, roi = cv.getOptimalNewCameraMatrix(mtx, dist, (w, h), 1, (w, h))\n",
    "\n",
    "\n",
    "def undistort_image(img):\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    img_undist = cv.undistort(img, mtx, dist, None, newcameramtx)\n",
    "    # crop the image\n",
    "    x, y, w, h = roi\n",
    "    return img_undist[y : y + h, x : x + w]\n",
    "\n",
    "\n",
    "# ~50% faster than undistort_image()\n",
    "def undistort_image_remap(img):\n",
    "    h, w = img.shape[:2]\n",
    "    mapx, mapy = cv.initUndistortRectifyMap(mtx, dist, None, newcameramtx, (w, h), 5)\n",
    "    dst = cv.remap(img, mapx, mapy, cv.INTER_LINEAR)\n",
    "    # crop the image\n",
    "    x, y, w, h = roi\n",
    "    return dst[y : y + h, x : x + w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perspektivtransformation\n",
    "\n",
    "Die Perspektivtransformation wird verwendet, um die \"Region of Interest\" festzulegen. Dadurch muss nicht mehr das gesamte Bild verarbeitet werden, sondern es kann nur ein der Bereich, in dem sich die Fahrspur befindet, zur Verarbeitung genutzt werden.\n",
    "\n",
    "Hierbei werden vier feste Punkte im originalen Bild ausgewählt. Diese Punkte werden dann in einem Bild mit den selben Dimensionen wie das Originalbild dargestellt, um eine rechteckige Beziehung zwischen den Punkten herzustellen (siehe Abbildung 1). Die Transformation geschieht mit der Funktion ```warp_image_udacity```.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/documentation/perspectiveTransformation.png\" />\n",
    "  <br>\n",
    "  <em>Abbildung 2: Beispiel für implementierte Perspektivtransformation</em>\n",
    "</p>\n",
    "\n",
    "Um die mit einer später erklärten Funktion (```sliding_windows```) in dem transformierten Bild gefundenen Punkte in dem originalen Bild anzeigen zu können, müssen diese Punkte zurücktransformiert werden. Um Rechenleistung zu sparen, werden tatsächlich nur die Punkte transformiert (siehe ```rewarp_points_udacity```), bei einem ersten Ansatz wurde das gesamte Bild zurücktransformiert (siehe ```rewarp_image_udacity```). \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# udacity images\n",
    "src_udacity = np.float32([[191, 628], [531, 404], [1021, 628], [681, 404]])\n",
    "dst_udacity = np.float32([[150, 720], [150, 10], [1000, 720], [1000, 10]])\n",
    "M_warp = cv.getPerspectiveTransform(src_udacity, dst_udacity)\n",
    "M_rewarp = cv.getPerspectiveTransform(dst_udacity, src_udacity)\n",
    "\n",
    "\n",
    "def warp_image_udacity(img):\n",
    "    img = cv.warpPerspective(img, M_warp, (img.shape[1], img.shape[0]))\n",
    "    # image = cv.resize(\n",
    "    #     image, (int(img.shape[1] / 2), int(img.shape[0] / 2))\n",
    "    # )  # resize to half size\n",
    "    return img\n",
    "\n",
    "def rewarp_image_udacity(img):\n",
    "    # image = cv.resize(\n",
    "    #     img, (int(img.shape[1] * 2), int(img.shape[0] * 2))\n",
    "    # ) \n",
    "    # img = cv.warpPerspective(img, M_rewarp, (img.shape[1], img.shape[0]))\n",
    "    img = cv.warpPerspective(img, M_rewarp, (img.shape[1], img.shape[0]), cv.WARP_INVERSE_MAP)\n",
    "    return img\n",
    "\n",
    "def rewarp_points_udacity(points):\n",
    "    return cv.perspectiveTransform(points, M_rewarp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sliding Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwindows = 10\n",
    "margin = 50\n",
    "def sliding_windows(frame, window_width=200, minimum_whites=30, show_windows=False):\n",
    "    # Histogram for image\n",
    "    hist = np.sum(frame[frame.shape[0]//2:, :], axis=0)\n",
    "        \n",
    "    # Take peaks from left and right side of histogramm for starting points and add half margin\n",
    "    mid = np.int32(hist.shape[0] // 2)\n",
    "    leftx_start = np.argmax(hist[:mid]) - window_width // 2\n",
    "    rightx_start = np.argmax(hist[mid:]) + mid + window_width // 2\n",
    "    # Window height based on number of windows\n",
    "    window_height = np.int32(frame.shape[0] // nwindows)\n",
    "    \n",
    "    # Calc points that are not zero in images\n",
    "    nonzero = frame.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    \n",
    "    # Initialize current positions for windows\n",
    "    leftx_current = leftx_start\n",
    "    rightx_current = rightx_start\n",
    "\n",
    "    # Initialize values to be returned -> centers of windows\n",
    "    lefts_good = np.empty((0,2), dtype=np.int32)\n",
    "    rights_good = np.empty((0,2), dtype=np.int32)\n",
    "\n",
    "    # Go through every window\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = frame.shape[0] - (window + 1) * window_height\n",
    "        win_y_high = frame.shape[0] - window*window_height\n",
    "        y_mid = (win_y_low + win_y_high) // 2\n",
    "        \n",
    "        # Calculate boundaries of the window\n",
    "        win_xleft_low = leftx_current - window_width  \n",
    "        win_xleft_high = leftx_current + window_width  \n",
    "        win_xright_low =  rightx_current - window_width \n",
    "        win_xright_high = rightx_current + window_width  \n",
    "        \n",
    "        # Identify the pixels that are not zero within window\n",
    "        left_inds = ((nonzeroy >= win_y_low ) & (nonzeroy < win_y_high) & (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        right_inds = ((nonzeroy >= win_y_low ) & (nonzeroy < win_y_high) & (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        \n",
    "        # If more than minimum pixels are found -> recenter next window\n",
    "        if len(left_inds) > minimum_whites:\n",
    "            leftx_current = np.int32(np.mean(nonzerox[left_inds]))\n",
    "            lefts_good = np.concatenate((lefts_good, [[leftx_current, y_mid]]))\n",
    "            if show_windows:\n",
    "                cv.rectangle(frame, (leftx_current - margin, win_y_low),(leftx_current + margin, win_y_high),(255, 255, 255), 2)\n",
    "        if len(right_inds) > minimum_whites:\n",
    "            rightx_current = np.int32(np.mean(nonzerox[right_inds]))\n",
    "            rights_good = np.concatenate((rights_good, [[rightx_current, y_mid]]))\n",
    "            if show_windows:\n",
    "                cv.rectangle(frame, (rightx_current - margin, win_y_low),(rightx_current + margin, win_y_high),(255, 255, 255), 2)\n",
    "\n",
    "    return mid, lefts_good, rights_good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynome\n",
    "\n",
    "Um eine Veränderung der Fahrspur, z.B. bei Kurven, angemessen darstellen zu können, werden anhand der von ```sliding_windows``` gefundenen Punkte Polynome berechnet.\n",
    "\n",
    "Diese bilden dann die rechten und linken Grenzen der einzufärbenden Fläche. Dadurch lassen sich selbst kleine Änderungend er Fahrspur ordentlich darstellen. (siehe Abbildung 3)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/documentation/polynoms.png\" width=\"40%\" height=\"40%\"/>\n",
    "  <br>\n",
    "  <em>Abbildung 3: Darstellung der berechneten Polynome auf der Fahrspur</em>\n",
    "</p>\n",
    "\n",
    "Mit Hilfe der Funktion ```drawRecOnFrame``` wird dann die Fläche zwischen den Polynomen eingezeichnet (siehe Abbildung 2).\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/documentation/filledLane.png\" width=\"40%\" height=\"40%\"/>\n",
    "  <br>\n",
    "  <em>Abbildung 4: Markierung der Fahrbahn mit Polynomen als Grenzen</em>\n",
    "</p>\n",
    "\n",
    "Um die Spurerkennung stabiler zu gestalten, werden bei einer zu geringen Anzahl an gefundenen Punkten keine neue Polynome berechnet. Die Spur wird dann anhand der zuletzt berechneten Polynome eingezeichnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  calculate polynomial from rewaped points\n",
    "def calculate_polynomial_points(lefts, rights):\n",
    "    \"\"\"\n",
    "    calculate polynomial from rewaped points and then return list of points on the polynomial\n",
    "    @return: array of points for left and right polynomial\n",
    "    \"\"\"\n",
    "    # calculate polynomial from rewaped points\n",
    "    left_polynom_values = np.polyfit(lefts[:,1], lefts[:,0], 2)\n",
    "    right_polynom_values = np.polyfit(rights[:,1], rights[:,0], 2)\n",
    "\n",
    "    # 750 as x lenght of polynom\n",
    "    x_axis = np.linspace(0, 750, 750) \n",
    "\n",
    "    # calculate y values for left and right line\n",
    "    left_line_y = left_polynom_values[0]*x_axis**2 + left_polynom_values[1]*x_axis + left_polynom_values[2]\n",
    "    right_line_y = right_polynom_values[0]*x_axis**2 + right_polynom_values[1]*x_axis + right_polynom_values[2]\n",
    "\n",
    "    # array of points for left and right line from x and y values\n",
    "    left_pts = np.array([np.transpose(np.vstack([left_line_y, x_axis]))])\n",
    "    right_pts = np.array([np.transpose(np.vstack([right_line_y, x_axis]))])\n",
    "\n",
    "    return left_pts, right_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawRecOnFrame(frame, left_pts, right_pts):\n",
    "    \"\"\"\n",
    "    input: array of points from left and right lane marking \n",
    "     e.g.:\n",
    "      [[ 281   39]\n",
    "      [ 971  163]\n",
    "      [ 958  101]]\n",
    "    \"\"\"\n",
    "    if len(left_pts) + len(right_pts) > 3:\n",
    "      borderPoints = np.concatenate((np.flip(left_pts, axis=0), right_pts))\n",
    "      borderPointsRewaped = rewarp_points_udacity(np.array([borderPoints], dtype=np.float32))\n",
    "      borderPointsRewapedInt = borderPointsRewaped.astype(int)\n",
    "      # draw in frame\n",
    "      cv.drawContours(frame, borderPointsRewapedInt, -1, (0,255,0), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking\n",
    "\n",
    "Mit ```applyMasks``` werden bestimmte Farbmasken, genauer eine für Weiß und eine für Gelb, auf das Bild gelegt. Damit werden nur diese zwei Farben in den Bildern hervorgehoben bzw. dargestellt. \n",
    "\n",
    "Diese Funktion ist inzwischen veraltet, da die Funktion ```lane_detection``` eine verbesserte Implementierung mit neuen Filtern zur Verfügung stellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyMasks(frame):\n",
    "    \"\"\"\n",
    "    apply masks to frame and return frame with only lane marking\n",
    "    \"\"\"\n",
    "    ## convert to hsv\n",
    "    hls_frame = cv.cvtColor(frame, cv.COLOR_BGR2HLS)\n",
    "\n",
    "    ## mask for white\n",
    "    white_mask = cv.inRange(hls_frame, (0, 200, 0), (255, 255,255))\n",
    "\n",
    "    ## mask for yellow\n",
    "    # yellow_mask = cv.inRange(hls_frame, (20,90,200), (26, 255, 255))\n",
    "    yellow_mask = cv.inRange(hls_frame, (10,0,100), (40, 255, 255))\n",
    "\n",
    "    ## final mask and masked\n",
    "    mask = cv.bitwise_or(white_mask, yellow_mask)\n",
    "    frame = cv.bitwise_and(frame,frame, mask=mask)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "Folgender Abschnitt zeigt den gesamten Ablauf der Fahrspurerkennung.\n",
    "\n",
    "In Zeile 2 lässt sich festlegen, in welchem der gegebenen Videos die Spurerkennung durchgeführt werden soll. Zur Auswahl stehen: \"project\", \"challenge\" und \"harder_challenge\".\n",
    "\n",
    "Nach einem Durchlauf des Programms findet sich am Ende des Notebooks die oben angesprochene Tabelle, die den zeitlichen Anteil der einzelnen Funktionen zeigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------+-----------+-----------+----------------------+\n",
      "| Name                   | Avg. [ms] | Min. [ms] | Max. [ms] | Occurrences [compl.] |\n",
      "+------------------------+-----------+-----------+-----------+----------------------+\n",
      "| frame                  |     34.13 |     27.01 |     75.58 |                   30 |\n",
      "| Preprocessing          |      7.93 |      6.05 |     12.12 |                   30 |\n",
      "| masking                |      7.37 |      5.87 |     11.86 |                   30 |\n",
      "| warping                |      3.57 |      2.81 |      5.41 |                   30 |\n",
      "| sliding windows        |     11.46 |      9.56 |     20.66 |                   30 |\n",
      "| polynomial calculation |      0.48 |      0.37 |      0.85 |                   30 |\n",
      "| draw plane             |      0.37 |      0.29 |      0.58 |                   30 |\n",
      "+------------------------+-----------+-----------+-----------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "# Open video file\n",
    "video_file = \"project\" # \"project\" or \"challenge\" or \"harder_challenge\"\n",
    "capture = cv.VideoCapture(\"./img/Udacity/\" + video_file + \"_video.mp4\")\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if capture.isOpened() == False:\n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "# Start timer for fps counter\n",
    "start_timer = time.time() - 0.01\n",
    "frame_count = -1\n",
    "\n",
    "# Read every frame\n",
    "while capture.isOpened():\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    # Check if there is another frame\n",
    "    if frame is None:\n",
    "        break\n",
    "    orig = frame.copy()\n",
    "\n",
    "    # Calculate Frame rate\n",
    "    frame_count += 1\n",
    "    ellapsed_time = time.time() - start_timer\n",
    "    frame_rate = frame_count / ellapsed_time\n",
    "\n",
    "    if ret == True:\n",
    "        start_time_measurement(\"frame\")\n",
    "\n",
    "        # ----------- Preprocessing ---------------\n",
    "        start_time_measurement(\"Preprocessing\")\n",
    "        frame = undistort_image_remap(frame)\n",
    "        orig_undist = frame.copy()\n",
    "        end_time_measurement(\"Preprocessing\")\n",
    "\n",
    "        # ---------  Masking ---------------------\n",
    "        start_time_measurement(\"masking\")\n",
    "        frame = applyMasks(frame)\n",
    "        end_time_measurement(\"masking\")\n",
    "\n",
    "        start_time_measurement(\"warping\")\n",
    "        frame = warp_image_udacity(frame)\n",
    "        end_time_measurement(\"warping\")\n",
    "\n",
    "        #---------- Sliding Windows ----------\n",
    "        start_time_measurement(\"sliding windows\")\n",
    "        # Convert to grayscale for sliding windows\n",
    "        grayscale_frame = cv.cvtColor(frame, cv.COLOR_HLS2BGR)\n",
    "        grayscale_frame = cv.cvtColor(grayscale_frame, cv.COLOR_BGR2GRAY)\n",
    "        midpoint, lefts, rights = sliding_windows(grayscale_frame, minimum_whites=margin, show_windows=False)\n",
    "        end_time_measurement(\"sliding windows\")\n",
    "  \n",
    "        #-------------------\n",
    "        grayscale_frame = orig_undist\n",
    "\n",
    "        #---------- Calculate polynomial ----------\n",
    "        start_time_measurement(\"polynomial calculation\")\n",
    "        left_pts, right_pts = calculate_polynomial_points(lefts, rights)\n",
    "        end_time_measurement(\"polynomial calculation\")\n",
    "\n",
    "        # -------- draw plane from all sliding windows ------------\n",
    "        start_time_measurement(\"draw plane\")\n",
    "        drawRecOnFrame(grayscale_frame, left_pts[0], right_pts[0])\n",
    "        end_time_measurement(\"draw plane\")\n",
    "\n",
    "        # -----------------\n",
    "        out = grayscale_frame\n",
    "     \n",
    "\n",
    "        # Add frame rate to video\n",
    "        cv.putText(out, \"FPS: \" + str(round(frame_rate)), (0, 25),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv.LINE_AA,)\n",
    "        cv.putText(out, \"Frame: \" + str(frame_count), (0, 50), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv.LINE_AA,)\n",
    "        cv.imshow(\"Frame\", out)\n",
    "\n",
    "        end_time_measurement(\"frame\")\n",
    "\n",
    "        # Close video with letter 'q'\n",
    "        if cv.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# When everything done, release the video capture object\n",
    "capture.release()\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "analyse_time_measurements()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2197ec28d7743a73ea97362e19eb170602465e026c4c8b6feaf2971891d5905a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
