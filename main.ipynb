{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projekt zur Spurerkennung im Wahlfach \"Digitale Bildverarbeitung\" \n",
    "\n",
    "In diesem Jupyter-Notebook wird eine Fahrbahnmarkierungserkennung implementiert. Anhand von Bildern und Videos von Udacity und KITTI wird diese Erkennung auf verschiedenste Weise getestet.\n",
    "\n",
    "Um eine Fahrbahn erkennen zu können müssen folgende Schritte während des Programmablaufs abgearbeitet werden:\n",
    "\n",
    "    - Kamerakalibrierung\n",
    "    - Perspektivtransformation\n",
    "    - Maskieren des Bildes mit gelben und weißen Farbmasken\n",
    "    - \"Sliding Windows\"\n",
    "    - Berechnung der Polynome für die Eingrenzung der Fahrbahn\n",
    "    - Rücktransformation der berechneten Punkte der Polynome\n",
    "    - Einzeichnen der Fläche zwischen den Polynomen\n",
    "\n",
    "Im folgenden werden die jeweils verwendeten Funktionen genauer erklärt, am Ende finden sich die Bilder zur Kamerakalibrierung sowie zur Spurerkennung.\n",
    "\n",
    "<sup>Bearbeitet wurde das Projekt von Alexander Schulte(), Edmund Krain () und Marcel Fleck (9611872)</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "from prettytable import PrettyTable\n",
    "from deprecated import deprecated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Measurement\n",
    "\n",
    "Diese Funktionen dienen der Bestimmung von zeitaufwändigen Arbeitsschritten, um eine Verbesserung der Framerate zu erleichtern.\n",
    "\n",
    "In einem Array wird ein Objekt hinterlegt, in dem der Name, die Start- und Endzeit sowie die Anzahl an Aufrufen hinterlegt wird (siehe ```start_time_measurement``` und ```end_time_measurement```).\n",
    "\n",
    "Diese Informationen werden dann in einer Tabelle nach Beendigung des Programmdurchlaufs visualisiert (siehe ```analyse_time_measurement```).\n",
    "\n",
    "Abbildung 1 zeigt ein Beispile für eine solche Tabelle:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/documentation/timemeasurement.png\" height=\"35%\" width=\"35%\" />\n",
    "  <br>\n",
    "  <em>Abbildung 1: Tabelle zur Darstellung der Zeitmessungsergebnisse</em>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_measurements = {}\n",
    "\n",
    "def start_time_measurement(eventName):\n",
    "    \"\"\"\n",
    "    Use this function to start a time measurement for a specific event.\n",
    "    A end_time_measurement() call with the same name must be called before the next start_time_measurement() call.\n",
    "    \"\"\"\n",
    "    add = False\n",
    "    if eventName not in time_measurements:\n",
    "        time_measurements[eventName] = {\n",
    "            \"name\": eventName,\n",
    "            \"start\": [],\n",
    "            \"end\": [],\n",
    "            \"count\": 0\n",
    "        }\n",
    "        add = True\n",
    "    elif len(time_measurements[eventName][\"start\"]) > len(time_measurements[eventName][\"end\"]):\n",
    "        print(f\"Time measure error: Event '{eventName}' not finished before reassignment!\")\n",
    "    else:\n",
    "       add = True\n",
    "    \n",
    "    #start measurement as late as possible\n",
    "    if add:\n",
    "        time_measurements[eventName][\"start\"].append(time.perf_counter_ns())\n",
    "\n",
    "def end_time_measurement(eventName):\n",
    "    \"\"\"\n",
    "    Use this function to end a time measurement for a specific event.\n",
    "    A time measurement with the same name must be started with start_time_measurement() before it can be ended.\n",
    "    \"\"\"\n",
    "    #end measurement as fast as possible\n",
    "    temp_time = time.perf_counter_ns()\n",
    "    if eventName not in time_measurements:\n",
    "        print(f\"Time measure error: Event '{eventName}' not defined!\")\n",
    "    elif len(time_measurements[eventName][\"end\"]) >= len(time_measurements[eventName][\"start\"]):\n",
    "        print(f\"Time measure error: Event '{eventName}' not started before reassignment!\")\n",
    "    else:\n",
    "        time_measurements[eventName][\"end\"].append(temp_time)\n",
    "        time_measurements[eventName][\"count\"] += 1\n",
    "    \n",
    "def analyse_time_measurements():\n",
    "    \"\"\"\n",
    "    Analyse time measurements and print them in a table\n",
    "    \"\"\"\n",
    "    time_measurements_table = PrettyTable([\"Name\", \"Avg. [ms]\", \"Min. [ms]\", \"Max. [ms]\", \"Occurrences [compl.]\"])\n",
    "    time_measurements_table.align[\"Name\"] = \"l\"\n",
    "    time_measurements_table.align[\"Avg. [ms]\"] = \"r\"\n",
    "    time_measurements_table.align[\"Min. [ms]\"] = \"r\"\n",
    "    time_measurements_table.align[\"Max. [ms]\"] = \"r\"\n",
    "    time_measurements_table.align[\"Occurrences [compl.]\"] = \"r\"\n",
    "    for key, event in time_measurements.items():\n",
    "        timings = []\n",
    "        if len(event[\"start\"]) != len(event[\"end\"]):\n",
    "            print(f\"Time measure error: Event '{key}' has different amounts of values for start and end times!\")\n",
    "        else:\n",
    "            #exclude 0 values\n",
    "            for i in range(len(event[\"start\"])):\n",
    "                timing = (event[\"end\"][i] - event[\"start\"][i])\n",
    "                if timing >= 0:\n",
    "                    timing = timing / (1000 * 1000) #convert from ns to ms\n",
    "                    timings.append(timing)\n",
    "\n",
    "            event[\"min\"] = min(timings) \n",
    "            event[\"max\"] = max(timings)\n",
    "            event[\"avg\"] = sum(timings) / len(event[\"start\"])\n",
    "\n",
    "            time_measurements_table.add_row([key, '{0:.2f}'.format(event[\"avg\"]), '{0:.2f}'.format(event[\"min\"]), '{0:.2f}'.format(event[\"max\"]), event[\"count\"]])\n",
    "    print(time_measurements_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kamerakalibrierung\n",
    "\n",
    "Die Abbildung eines 3D-Objekts auf eine 2D-Bildebene mit einer Kamera wird durch die internen Größen einer Kamera beeinflusst. Dazu gehören zum Beispiel die Bildmitte, die Brennweite und auch Kameraverzerrungsparameter.\n",
    "\n",
    "Mit Hilfe der Kamerakalibrierung werden die sogenannten \"intrinsische\" und \"extrinsische\" Parameter berechnet. Intrinsische Parameter beschreiben die Kalibriermatrix (Kamera zu Pixel), während die extrinsischen Parameter die Beziehung zwischen dem Koordinatensystem der Kamera und dem Welt-Koordinatensystem beschreiben (Welt zu Kamera).\n",
    "\n",
    "Zusammen bilden diese Parameter eine Projektionsmatrix, die es ermöglicht, gemachte Bilder im Welt-Koordinatensystem darzustellen (die Bilder werden entzerrt). \n",
    "\n",
    "In folgendem Abschnitt wird die Kamera mit Hilfe von Schachbrett-Bildern kalibriert. Diese Aufgabe mit den Ergebnissen auf den Schachbrettbildern befinden sich im JupyterNotebook [Projekt_Spurerkennung_v2.jpynb](Projekt_Spurerkennung_v2.ipynb).\n",
    "\n",
    "\n",
    "Mit den unten definierten Funktionen (siehe ```undistort_image``` und ```undistort_image_remap```) werden dann die Frames der Videos anhand der berechneten Transformations-Matrix entzerrt, um eine tatsächliche Darstellung im Welt-Koordinatensystem zu erhalten. Eine Implementierung von zwei Funktionen lässt mit dem Erreichen von einer ungefähr 50% schnelleren Verarbeitung begründen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of chessboard, minimum error with (7, 6), but there were severe artefacts at the borders (see error calculation at the end)\n",
    "chessboard_x, chessboard_y = 9, 6\n",
    "\n",
    "# termination criteria\n",
    "criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "object_points = np.zeros((chessboard_y * chessboard_x, 3), np.float32)\n",
    "object_points[:, :2] = np.mgrid[0:chessboard_x, 0:chessboard_y].T.reshape(-1, 2)\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = []  # 3d point in real world space\n",
    "imgpoints = []  # 2d points in image plane.\n",
    "\n",
    "# use all calibration images\n",
    "images = glob.glob(\"./img/Udacity/calib/*.jpg\")\n",
    "for i, frame_name in enumerate(images):\n",
    "    image = cv.imread(frame_name)\n",
    "    image_gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find the chess board corners\n",
    "    ret, corners = cv.findChessboardCorners(image_gray, (chessboard_x, chessboard_y), None)\n",
    "    # If found, add object points, image points (after refining them)\n",
    "    if ret == True:\n",
    "        objpoints.append(object_points)\n",
    "        corners2 = cv.cornerSubPix(\n",
    "            image_gray, corners, (11, 11), (-1, -1), criteria\n",
    "        )  # improve accuracy of corners\n",
    "        imgpoints.append(corners)\n",
    "\n",
    "ret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(\n",
    "    objpoints, imgpoints, image_gray.shape[::-1], None, None\n",
    ")\n",
    "\n",
    "image_height, image_width = image.shape[:2]\n",
    "newcameramtx, roi = cv.getOptimalNewCameraMatrix(mtx, dist, (image_width, image_height), 1, (image_width, image_height))\n",
    "\n",
    "\n",
    "# ------- define functions for image processing -------\n",
    "def undistort_image(img):\n",
    "    image_height, image_width = img.shape[:2]\n",
    "\n",
    "    img_undist = cv.undistort(img, mtx, dist, None, newcameramtx)\n",
    "    # crop the image\n",
    "    x, y, image_width, image_height = roi\n",
    "    return img_undist[y : y + image_height, x : x + image_width]\n",
    "\n",
    "# ~50% faster than undistort_image()\n",
    "def undistort_image_remap(img):\n",
    "    image_height, image_width = img.shape[:2]\n",
    "    mapx, mapy = cv.initUndistortRectifyMap(mtx, dist, None, newcameramtx, (image_width, image_height), 5)\n",
    "    dst = cv.remap(img, mapx, mapy, cv.INTER_LINEAR)\n",
    "    # crop the image\n",
    "    x, y, image_width, image_height = roi\n",
    "    return dst[y : y + image_height, x : x + image_width]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perspektivtransformation\n",
    "\n",
    "Die Perspektivtransformation wird verwendet, um die \"Region of Interest\" festzulegen. Dadurch muss nicht mehr das gesamte Bild verarbeitet werden, sondern es kann nur ein der Bereich, in dem sich die Fahrspur befindet, zur Verarbeitung genutzt werden.\n",
    "\n",
    "Hierbei werden vier feste Punkte im originalen Bild ausgewählt. Diese Punkte werden dann in einem Bild mit den selben Dimensionen wie das Originalbild dargestellt, um eine rechteckige Beziehung zwischen den Punkten herzustellen (siehe Abbildung 1). Die Transformation geschieht mit der Funktion ```warp_image_udacity```.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/documentation/perspectiveTransformation.png\" />\n",
    "  <br>\n",
    "  <em>Abbildung 2: Beispiel für implementierte Perspektivtransformation</em>\n",
    "</p>\n",
    "\n",
    "Um die mit einer später erklärten Funktion (```sliding_windows```) in dem transformierten Bild gefundenen Punkte in dem originalen Bild anzeigen zu können, müssen diese Punkte zurücktransformiert werden. Um Rechenleistung zu sparen, werden tatsächlich nur die Punkte transformiert (siehe ```rewarp_points_udacity```), bei einem ersten Ansatz wurde das gesamte Bild zurücktransformiert (siehe ```rewarp_image_udacity```). \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# udacity images\n",
    "src_udacity = np.float32([[191, 628], [531, 404], [1021, 628], [681, 404]])\n",
    "dst_udacity = np.float32([[150, 720], [150, 10], [1000, 720], [1000, 10]])\n",
    "M_warp = cv.getPerspectiveTransform(src_udacity, dst_udacity)\n",
    "M_rewarp = cv.getPerspectiveTransform(dst_udacity, src_udacity)\n",
    "\n",
    "\n",
    "def warp_image_udacity(img):\n",
    "    img = cv.warpPerspective(img, M_warp, (img.shape[1], img.shape[0]))\n",
    "    # image = cv.resize(\n",
    "    #     image, (int(img.shape[1] / 2), int(img.shape[0] / 2))\n",
    "    # )  # resize to half size\n",
    "    return img\n",
    "\n",
    "@deprecated\n",
    "def rewarp_image_udacity(img):\n",
    "    # image = cv.resize(\n",
    "    #     img, (int(img.shape[1] * 2), int(img.shape[0] * 2))\n",
    "    # ) \n",
    "    # img = cv.warpPerspective(img, M_rewarp, (img.shape[1], img.shape[0]))\n",
    "    img = cv.warpPerspective(img, M_rewarp, (img.shape[1], img.shape[0]), cv.WARP_INVERSE_MAP)\n",
    "    return img\n",
    "\n",
    "def rewarp_points_udacity(points):\n",
    "    return cv.perspectiveTransform(points, M_rewarp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sliding Windows\n",
    "\n",
    "Die Erkennung der Fahrbahnmarkierungen wird mit den sogenannten \"Sliding Windows\" umgesetzt. \n",
    "\n",
    "Kurz gefasst sind \"Sliding Windows\" kleine Kästchen, die anhand der Helligkeit von Pixel ihren Mittelpunkt anpassen. Damit kann beispielsweise eine weiße Linie auf schwarzem Hintergrund fast perfekt verfolgt werden. \n",
    "\n",
    "Detailreicher beginnt die ```sliding_windows``` Funktion damit, anhand des Histogramms eines Frames die initialen Mittelpunkte der ersten Windows festzulegen. Anhand dieser Mittelpunkte werden eine angegebene Anzahl an Windows erzeugt, die aufeinander gestapelt werden. Das bedeutet, der X-Wert der Windows bleibt gleich, der Y-Wert passt sich an.\n",
    "\n",
    "Für jedes Window wird nun geprüft, wie viele hellen Pixel von dem jeweiligen Window eingegrenzt werden. Wenn der Wert der gefundenen Pixel das angegebene Minimum unterschreitet, wird der Mittelpunkt des jeweiligen Windows angepasst.\n",
    "\n",
    "Ein Beispiel für diese \"Sliding Windows\" zeigt Abbildung 3.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/documentation/slidingWindows.png\" width=\"40%\" height=\"40%\"/>\n",
    "  <br>\n",
    "  <em>Abbildung 3: Beispiel für die Darstellung der \"Sliding Windows\"</em>\n",
    "</p>\n",
    "\n",
    "Damit kann die Fahrspur bei einer Veränderung leicht weiterverfolgt und markiert werden. Die hier gefundenen Mittelpunkte bilden die Grundlage für die Polynomberechnung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwindows = 50\n",
    "margin = 50\n",
    "def sliding_windows(frame, window_width=200, minimum_whites=30, show_windows=False):\n",
    "    # Histogram for image\n",
    "    hist = np.sum(frame[frame.shape[0]//2:, :], axis=0)\n",
    "        \n",
    "    # Take peaks from left and right side of histogramm for starting points and add half margin\n",
    "    mid = np.int32(hist.shape[0] // 2)\n",
    "    leftx_start = np.argmax(hist[:mid]) - window_width // 2\n",
    "    rightx_start = np.argmax(hist[mid:]) + mid + window_width // 2\n",
    "    # Window height based on number of windows\n",
    "    window_height = np.int32(frame.shape[0] // nwindows)\n",
    "    \n",
    "    # Calc points that are not zero in images\n",
    "    nonzero = frame.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    \n",
    "    # Initialize current positions for windows\n",
    "    leftx_current = leftx_start\n",
    "    rightx_current = rightx_start\n",
    "\n",
    "    # Initialize values to be returned -> centers of windows\n",
    "    lefts_good = np.empty((0,2), dtype=np.int32)\n",
    "    rights_good = np.empty((0,2), dtype=np.int32)\n",
    "\n",
    "    # Go through every window\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = frame.shape[0] - (window + 1) * window_height\n",
    "        win_y_high = frame.shape[0] - window*window_height\n",
    "        y_mid = (win_y_low + win_y_high) // 2\n",
    "        \n",
    "        # Calculate boundaries of the window\n",
    "        win_xleft_low = leftx_current - window_width  \n",
    "        win_xleft_high = leftx_current + window_width  \n",
    "        win_xright_low =  rightx_current - window_width \n",
    "        win_xright_high = rightx_current + window_width  \n",
    "        \n",
    "        # Identify the pixels that are not zero within window\n",
    "        left_inds = ((nonzeroy >= win_y_low ) & (nonzeroy < win_y_high) & (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        right_inds = ((nonzeroy >= win_y_low ) & (nonzeroy < win_y_high) & (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        \n",
    "        # If more than minimum pixels are found -> recenter next window\n",
    "        if len(left_inds) > minimum_whites:\n",
    "            leftx_current = np.int32(np.mean(nonzerox[left_inds]))\n",
    "            lefts_good = np.concatenate((lefts_good, [[leftx_current, y_mid]]))\n",
    "            if show_windows:\n",
    "                cv.rectangle(frame, (leftx_current - margin, win_y_low),(leftx_current + margin, win_y_high),(255, 255, 255), 2)\n",
    "        if len(right_inds) > minimum_whites:\n",
    "            rightx_current = np.int32(np.mean(nonzerox[right_inds]))\n",
    "            rights_good = np.concatenate((rights_good, [[rightx_current, y_mid]]))\n",
    "            if show_windows:\n",
    "                cv.rectangle(frame, (rightx_current - margin, win_y_low),(rightx_current + margin, win_y_high),(255, 255, 255), 2)\n",
    "\n",
    "    return mid, lefts_good, rights_good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynome\n",
    "\n",
    "Um eine Veränderung der Fahrspur, z.B. bei Kurven, angemessen darstellen zu können, werden anhand der von ```sliding_windows``` gefundenen Punkte Polynome berechnet.\n",
    "\n",
    "Diese bilden dann die rechten und linken Grenzen der einzufärbenden Fläche. Dadurch lassen sich selbst kleine Änderungend er Fahrspur ordentlich darstellen. (siehe Abbildung 4)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/documentation/polynoms.png\" width=\"40%\" height=\"40%\"/>\n",
    "  <br>\n",
    "  <em>Abbildung 4: Darstellung der berechneten Polynome auf der Fahrspur</em>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_polynom_curvature_value = 0.005 # defines how much the polynom can be curved before it is considered faulty\n",
    "\n",
    "def calculate_polynomial_points(lefts, rights):\n",
    "    \"\"\"\n",
    "    calculate polynomial from rewarped points and then return list of points on the polynomial\n",
    "    @return: array of points for left and right polynomial\n",
    "    \"\"\"\n",
    "    # flags for faulty polynom sides\n",
    "    is_left_faulty, is_right_faulty = False, False\n",
    "    \n",
    "    # check if there are enough points\n",
    "    if len(lefts) < 2 or len(rights) < 2:\n",
    "        # print(\"Not enough points to calculate polynomial\")\n",
    "        return None, None\n",
    "    \n",
    "    # calculate polynomial from rewarped points\n",
    "    left_polynom_values = np.polyfit(lefts[:,1], lefts[:,0], 2)\n",
    "    right_polynom_values = np.polyfit(rights[:,1], rights[:,0], 2)\n",
    "\n",
    "    # check if polynom is valid (not too steeply curved)\n",
    "    if (left_polynom_values[0] > max_polynom_curvature_value or left_polynom_values[0] < -max_polynom_curvature_value):\n",
    "        is_left_faulty = True\n",
    "    if (right_polynom_values[0] > max_polynom_curvature_value or right_polynom_values[0] < -max_polynom_curvature_value):\n",
    "        is_right_faulty = True\n",
    "    if is_left_faulty and is_right_faulty:\n",
    "        return None, None\n",
    "    \n",
    "    # 750 as x length of polynom\n",
    "    x_axis = np.linspace(0, 750, 750)\n",
    "\n",
    "    # calculate y values for left and right line\n",
    "    left_line_y = left_polynom_values[0]*x_axis**2 + left_polynom_values[1]*x_axis + left_polynom_values[2]\n",
    "    right_line_y = right_polynom_values[0]*x_axis**2 + right_polynom_values[1]*x_axis + right_polynom_values[2]\n",
    "\n",
    "    # array of points for left and right line from x and y values\n",
    "    left_pts = np.array([np.transpose(np.vstack([left_line_y, x_axis]))])\n",
    "    right_pts = np.array([np.transpose(np.vstack([right_line_y, x_axis]))])\n",
    "    \n",
    "    # loop through all points and check if they are to close or to far away from each other\n",
    "    # (if they are to close -> most likely a interception point)\n",
    "    for i in range(len(left_pts[0])):\n",
    "        y_distance = abs(left_pts[0][i][0] - right_pts[0][i][0])\n",
    "        if y_distance < 10 or y_distance > 1000:\n",
    "            return None, None\n",
    "\n",
    "    # only return non faulty polynom points\n",
    "    return left_pts if not is_left_faulty else None, right_pts if not is_right_faulty else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enzeichen der Fahrspur\n",
    "\n",
    "Mit Hilfe der Funktion ```drawRecOnFrame``` wird dann die Fläche zwischen den Polynomen eingezeichnet (siehe Abbildung 5).\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"img/documentation/filledLane.png\" width=\"40%\" height=\"40%\"/>\n",
    "  <br>\n",
    "  <em>Abbildung 5: Markierung der Fahrbahn mit Polynomen als Grenzen</em>\n",
    "</p>\n",
    "\n",
    "Um die Spurerkennung stabiler zu gestalten, werden bei einer zu geringen Anzahl an gefundenen Punkten keine neue Polynome berechnet. Die Spur wird dann anhand der zuletzt berechneten Polynome eingezeichnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawRecOnFrame(frame, left_pts, right_pts):\n",
    "    \"\"\"\n",
    "    input: array of points from left and right lane marking \n",
    "     e.g.:\n",
    "      [[ 281   39]\n",
    "      [ 971  163]\n",
    "      [ 958  101]]\n",
    "    \"\"\"\n",
    "    if len(left_pts) + len(right_pts) > 3:\n",
    "      borderPoints = np.concatenate((np.flip(left_pts, axis=0), right_pts))\n",
    "      borderPointsRewaped = rewarp_points_udacity(np.array([borderPoints], dtype=np.float32))\n",
    "      borderPointsRewapedInt = borderPointsRewaped.astype(int)\n",
    "      # draw in frame\n",
    "      cv.drawContours(frame, borderPointsRewapedInt, -1, (0,255,0), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking\n",
    "\n",
    "Mit ```applyMasks``` werden bestimmte Farbmasken, genauer eine für Weiß und eine für Gelb, auf das Bild gelegt. Damit werden nur diese zwei Farben in den Bildern hervorgehoben bzw. dargestellt. \n",
    "\n",
    "Diese Funktion ist inzwischen veraltet, da die Funktion ```lane_detection``` eine verbesserte Implementierung mit neuen Filtern zur Verfügung stellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@deprecated\n",
    "def applyMasks(frame):\n",
    "    \"\"\"\n",
    "    apply masks to frame and return frame with only lane marking\n",
    "    \"\"\"\n",
    "    ## convert to hsv\n",
    "    hls_frame = cv.cvtColor(frame, cv.COLOR_BGR2HLS)\n",
    "\n",
    "    ## mask for white\n",
    "    white_mask = cv.inRange(hls_frame, (0, 200, 0), (255, 255,255))\n",
    "\n",
    "    ## mask for yellow\n",
    "    # yellow_mask = cv.inRange(hls_frame, (20,90,200), (26, 255, 255))\n",
    "    yellow_mask = cv.inRange(hls_frame, (10,0,100), (40, 255, 255))\n",
    "\n",
    "    ## final mask and masked\n",
    "    mask = cv.bitwise_or(white_mask, yellow_mask)\n",
    "    frame = cv.bitwise_and(frame,frame, mask=mask)\n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lane_detection(frame, flag=False):\n",
    "    frame = cv.GaussianBlur(frame, (5, 5), 0)\n",
    "    frame_hls = cv.cvtColor(frame, cv.COLOR_BGR2HLS)\n",
    "    white_lower = (0, 180, 0)\n",
    "    white_upper = (254, 254, 254)\n",
    "    if flag:\n",
    "        white_lower = (0, 180, 0)\n",
    "        white_upper = (240, 240, 240)\n",
    "\n",
    "    white_mask = cv.inRange(frame_hls, white_lower, white_upper)\n",
    "\n",
    "    frame_lab = cv.cvtColor(frame, cv.COLOR_BGR2LAB)\n",
    "    yellow_mask = cv.inRange(frame_lab, (150, 100, 140), (255, 140, 200))\n",
    "\n",
    "    combined_mask = cv.bitwise_or(white_mask, yellow_mask)\n",
    "\n",
    "    redImg = np.zeros(frame.shape, frame.dtype)\n",
    "    redImg[:,:] = (0, 0, 255)\n",
    "    redMask = cv.bitwise_and(redImg, redImg, mask=yellow_mask)\n",
    "\n",
    "    greenImg = np.zeros(frame.shape, frame.dtype)\n",
    "    greenImg[:,:] = (0, 255, 0)\n",
    "    greenMask = cv.bitwise_and(greenImg, greenImg, mask=white_mask)\n",
    "\n",
    "    masks = redMask + greenMask\n",
    "    # if not flag:\n",
    "    #     frame_luv = cv.cvtColor(frame, cv.COLOR_BGR2LUV)\n",
    "    #     yellow_mask2 = cv.inRange(frame_luv, (150, 100, 150), (255, 140, 200))\n",
    "    #     combined_yellow = cv.bitwise_and(yellow_mask, yellow_mask2)\n",
    "    #     combined_mask = cv.bitwise_or(white_mask, combined_yellow)\n",
    "    #     blueImg = np.zeros(frame.shape, frame.dtype)\n",
    "    #     blueImg[:,:] = (255, 0, 0)\n",
    "    #     blueMask = cv.bitwise_and(blueImg, blueImg, mask=yellow_mask2)\n",
    "    #     masks = masks + blueMask\n",
    "\n",
    "    kernel = cv.getStructuringElement(cv.MORPH_ELLIPSE, (7, 3))\n",
    "    lanes = cv.morphologyEx(frame_hls[:, :, 1], cv.MORPH_TOPHAT, kernel)\n",
    "\n",
    "    kernel = cv.getStructuringElement(cv.MORPH_RECT, (13, 13))\n",
    "    lanes_yellow = cv.morphologyEx(frame_lab[:, :, 2], cv.MORPH_TOPHAT, kernel)\n",
    "\n",
    "    ret, lanes_yellow = cv.threshold(lanes_yellow, thresh=2, maxval=255, type=cv.THRESH_BINARY)\n",
    "    lanes_yellow = cv.morphologyEx(lanes_yellow, cv.MORPH_OPEN, np.array([[0,1,0],[1,1,1],[0,1,0]], 'uint8'), iterations=4)\n",
    "\n",
    "    lanes = cv.morphologyEx(frame_hls[:, :, 1], \n",
    "                              cv.MORPH_BLACKHAT,\n",
    "                              kernel)\n",
    "\n",
    "    ret, lanes = cv.threshold(lanes, thresh=5, maxval=255, type=cv.THRESH_BINARY)\n",
    "\n",
    "    combined_mask = cv.bitwise_or(combined_mask, lanes_yellow)\n",
    "\n",
    "    combined_mask = cv.morphologyEx(combined_mask, cv.MORPH_OPEN, np.array([[0,1,0],[1,1,1],[0,1,0]], 'uint8'), iterations=2)\n",
    "    # combined_mask = cv.Canny(combined_mask, 50, 150)\n",
    "    frame = cv.bitwise_and(frame, frame, mask=combined_mask)\n",
    "    frame = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "    \n",
    "    cv.imshow(\"Masks\", masks)\n",
    "    cv.imshow(\"TOP_HAT_LANES_YELLOW\", lanes_yellow)\n",
    "    cv.imshow(\"BLACK_HAT_LANES\", lanes)\n",
    "    cv.imshow(\"Final\", combined_mask)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helligkeitskorrektur\n",
    "\n",
    "Damit die Fahrbahnmarkierungen auch in dunklen Elementen der Videos zu erkennen sind, wurde eine dynamische Helligkeitsanpassung implementiert.\n",
    "\n",
    "Das Bild wird in HSV eingelesen und in die jeweiligen Bestandteile aufgeteilt. Wenn der Helligkeitswert des Frames dann kleiner oder größer ist als die angegebenen Grenzen, dann wird die Helligkeit des Bilds so angepasst, dass sie sich dann zwischen den gesetzten Grenzen befindet (vgl. Abbildung 5).\n",
    "\n",
    "<p align=\"middle\" float=\"left\">\n",
    "  <img src=\"img/documentation/brightnessCorrectionNormal.png\" width=\"40%\" height=\"40%\"/>\n",
    "  <img src=\"img/documentation/brightnessCorrectionCorrected.png\" width=\"40%\" height=\"40%\"/>\n",
    "  <br>\n",
    "  <em>Abbildung 5: Helligkeitskorrektur - Links: Original, Rechts: Korrigiert</em>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_Brightness(frame):\n",
    "    frame_hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n",
    "    brightness = frame_hsv[...,2].mean()\n",
    "    brightness_upper_limit = 115\n",
    "    brightness_lower_limit = 85\n",
    "    flag = False\n",
    "    if brightness > brightness_upper_limit:\n",
    "        hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n",
    "        h, s, v = cv.split(hsv)\n",
    "        \n",
    "        lim = round(0 + brightness - brightness_upper_limit)\n",
    "        v[v < lim] = 0\n",
    "        v[v >= lim] -= lim\n",
    "\n",
    "        final_hsv = cv.merge((h, s, v))\n",
    "        frame = cv.cvtColor(final_hsv, cv.COLOR_HSV2BGR)\n",
    "        flag = True\n",
    "    \n",
    "    if brightness < brightness_lower_limit:\n",
    "        hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n",
    "        h, s, v = cv.split(hsv)\n",
    "        \n",
    "        lim = round(255 + brightness_lower_limit - brightness)\n",
    "        v[v > lim] = 255\n",
    "        v[v <= lim] += lim\n",
    "\n",
    "        final_hsv = cv.merge((h, s, v))\n",
    "        frame = cv.cvtColor(final_hsv, cv.COLOR_HSV2BGR)\n",
    "        flag = False\n",
    "\n",
    "    return frame, flag, brightness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "Folgender Abschnitt zeigt den gesamten Ablauf der Fahrspurerkennung.\n",
    "\n",
    "In Zeile 2 lässt sich festlegen, in welchem der gegebenen Videos die Spurerkennung durchgeführt werden soll. Zur Auswahl stehen: \"project\", \"challenge\" und \"harder_challenge\".\n",
    "\n",
    "Nach einem Durchlauf des Programms findet sich am Ende des Notebooks die oben angesprochene Tabelle, die den zeitlichen Anteil der einzelnen Funktionen zeigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open video file\n",
    "# video_file = \"project\"\n",
    "video_file = \"challenge\"\n",
    "# video_file = \"harder_challenge\"\n",
    "capture = cv.VideoCapture(\"./img/Udacity/\" + video_file + \"_video.mp4\")\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if capture.isOpened() == False:\n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "# Start timer for fps counter\n",
    "start_timer = time.time() - 0.01\n",
    "frame_count = -1\n",
    "\n",
    "# cache between frames\n",
    "old_left_pts = None\n",
    "old_right_pts = None\n",
    "\n",
    "# reset time analysis\n",
    "time_measurements = {}\n",
    "\n",
    "# Read every frame\n",
    "while capture.isOpened():\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    # Check if there is another frame\n",
    "    if frame is None:\n",
    "        break\n",
    "\n",
    "    # Calculate Frame rate\n",
    "    frame_count += 1\n",
    "    elapsed_time = time.time() - start_timer\n",
    "    frame_rate = frame_count / elapsed_time\n",
    "    \n",
    "    if ret == True:\n",
    "        start_time_measurement(\"frame\")\n",
    "        \n",
    "        # ----------- Preprocessing ---------------\n",
    "        start_time_measurement(\"Preprocessing\")\n",
    "        frame = undistort_image_remap(frame)\n",
    "        frame_undistorted = frame.copy()\n",
    "        frame = warp_image_udacity(frame)\n",
    "        end_time_measurement(\"Preprocessing\")\n",
    "        \n",
    "        # ---------  Masking ---------------------\n",
    "        start_time_measurement(\"masking\")\n",
    "        # frame = applyMasks(frame)\n",
    "        frame, flag, brightness = correct_Brightness(frame)\n",
    "        grayscale_frame = lane_detection(frame, flag)\n",
    "        end_time_measurement(\"masking\")\n",
    "\n",
    "        #---------- Sliding Windows ----------\n",
    "        start_time_measurement(\"sliding windows\")\n",
    "        # Convert to grayscale for sliding windows\n",
    "        midpoint, lefts, rights = sliding_windows(grayscale_frame, minimum_whites=margin, show_windows=False)\n",
    "        end_time_measurement(\"sliding windows\")\n",
    "\n",
    "        #---------- Calculate polynomial ----------\n",
    "        start_time_measurement(\"polynomial calculation\")\n",
    "        left_pts, right_pts = calculate_polynomial_points(lefts, rights)\n",
    "        end_time_measurement(\"polynomial calculation\")\n",
    "\n",
    "        # -------- draw plane from all sliding windows ------------\n",
    "        start_time_measurement(\"draw plane\")\n",
    "        if left_pts is not None:\n",
    "            old_left_pts = left_pts        \n",
    "        if right_pts is not None:\n",
    "            old_right_pts = right_pts\n",
    "        drawRecOnFrame(frame_undistorted, old_left_pts[0], old_right_pts[0])\n",
    "        if old_left_pts is  None and old_right_pts is  None:\n",
    "            print(\"No plane drawn\")\n",
    "        end_time_measurement(\"draw plane\")\n",
    "\n",
    "        # -------- Add frame rate to video ------------\n",
    "        cv.putText(frame_undistorted, \"FPS: \" + str(round(frame_rate)), (0, 25),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv.LINE_AA,)\n",
    "        cv.putText(frame_undistorted, \"Frame: \" + str(frame_count), (0, 50), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv.LINE_AA,)\n",
    "        cv.putText(frame_undistorted, \"Brightness: \" + str(np.round(brightness)), (0, 75), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv.LINE_AA,)\n",
    "        cv.imshow(\"Frame\", frame_undistorted)\n",
    "\n",
    "\n",
    "        end_time_measurement(\"frame\")\n",
    "        # Close video with letter 'q'\n",
    "        if cv.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# When everything done, release the video capture object\n",
    "capture.release()\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "# create time measurement analysis and print out results\n",
    "analyse_time_measurements()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2197ec28d7743a73ea97362e19eb170602465e026c4c8b6feaf2971891d5905a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
