{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Projekt zur Spurerkennung im Wahlfach \"Digitale Bildverarbeitung\" \n",
        "\n",
        "In diesem Jupyter-Notebook wird eine Fahrbahnmarkierungserkennung implementiert. Anhand von Bildern und Videos von Udacity und KITTI wird diese Erkennung auf verschiedenste Weise getestet.\n",
        "\n",
        "Um eine Fahrbahn erkennen zu können müssen folgende Schritte während des Programmablaufs abgearbeitet werden:\n",
        "\n",
        "    - Kamerakalibrierung\n",
        "    - Perspektivtransformation\n",
        "    - Maskieren des Bildes mit gelben und weißen Farbmasken\n",
        "    - \"Sliding Windows\"\n",
        "    - Berechnung der Polynome für die Eingrenzung der Fahrbahn\n",
        "    - Rücktransformation der berechneten Punkte der Polynome\n",
        "    - Einzeichnen der Fläche zwischen den Polynomen\n",
        "\n",
        "Im folgenden werden die jeweils verwendeten Funktionen genauer erklärt, am Ende finden sich die Bilder zur Kamerakalibrierung sowie zur Spurerkennung.\n",
        "\n",
        "<sup>Bearbeitet wurde das Projekt von Alexander Schulte(), Edmund Krain () und Marcel Fleck (9611872)</sup>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import time\n",
        "import glob\n",
        "from prettytable import PrettyTable\n",
        "from deprecated import deprecated"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wichtige Attribute/Flags\n",
        "\n",
        "Hier kann gewählt werden, welches Video für die Spurerkennung verwendet werden soll. Zur Auswahl stehen das \"project\"-Video sowie das \"challenge\"-Video.\n",
        "\n",
        "Die Flag ```DEBUG_MODE``` dient zur Aktivierung des von uns implementierten Debug-Modus. Dabei werden mehrere Fenster mit unterschiedlichen Masken und Einstellungsmöglichkeiten angezeigt, um die optimalen Einstellungen durch ausprobieren herausfinden zu können. Zu den jeweiligen Debug-Optionen folgt in den jeweiligen Abschnitten mehr. Da mit der Aktivierung des Debug-Modus das Video nicht mehr automatisch läuft sondern manuell einzelne Frames durchlaufen werden können, ist der Debug-Modus standardweise ausgeschalten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# video_file = \"project\"\n",
        "video_file = \"challenge\"\n",
        "\n",
        "DEBUG_MODE = False"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Zeitmessungen\n",
        "\n",
        "Diese Funktionen dienen der Bestimmung von zeitaufwändigen Arbeitsschritten, um eine Verbesserung der Framerate zu erleichtern.\n",
        "\n",
        "Bei jedem Aufruf der ```start_time_measurement```-Funktion wird in einem Array ein Objekt mit den Eigenschaften \"name\", \"start\", \"end\" und \"count\" erstellt.\n",
        "Der Name dient als hierbei als eindeutige Identifizierung der Objekte.\n",
        "\n",
        "Nach der Erstellung des Objekts wird die aktuelle Zeit nanosekundengenau gespeichert. Da diese Zeit so spät wie nur möglich abgespeichert wird, sind die Messergebnisse nicht von den Operationen der Funktion beeinträchtigt.\n",
        "Die gespeicherte Zeit wird nun in das \"start\"-Attribut des Objektes hinzugefügt. Sie wird für die spätere Berechnung der Dauer benötigt.\n",
        "\n",
        "Bei einem Aufruf von ```end_time_measurement``` wird zuallererst die aktuelle Zeit, wieder auf Nanosekunden genau, abgespeichert. Hier wird die Zeit als erster Arbeitsschritt gespeichert, um wieder die Messergebnisse nicht durch die weiteren Operationen der Funktion zu beeinträchtigen. Diese Zeit wird dann im \"end\"-Attribut des Objekts gespeichert.\n",
        "\n",
        "Falls ```start_time_management``` mit einem Namen aufgerufen wird, für den es bereits einen Eintrag im Arry gibt, wird eine Variable hochgezählt. Mit Hilfe dieser lässt sich später die durchschnittliche Dauer der Operationen berechnen.\n",
        "\n",
        "Beide Funktionen beinhalten Abfragen, die ein doppeltes Starten oder auch ein Beenden vor einem Start verhindern.\n",
        "\n",
        "```analyse_time_measurement``` stellt die gesammelten Daten in einer Tabelle an.\n",
        "Hierfür werden erst die durchschnittliche, die minimale und die maximale Dauer der jeweiligen, per Namen definierten Objekte berechnet und von Nanosekunden zu Millisekunden umgerechnet.\n",
        "Auch die Anzahl der Aufrufe wird in der Tabelle angezeigt.\n",
        "\n",
        "Abbildung 1 zeigt ein Beispiel für eine solche Tabelle:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"img/documentation/timemeasurement.png\" height=\"35%\" width=\"35%\" />\n",
        "  <br>\n",
        "  <em>Abbildung 1: Tabelle zur Darstellung der Zeitmessungsergebnisse</em>\n",
        "</p>\n",
        "\n",
        "Im Allgemeinen werden diese Funktionen jeweils vor und nach wichtigen Arbeitsschritten in der Spurerkennung (wie z.B. das Maskieren der Frames) aufgerufen, um feststellen zu können, ob die jeweiligen Schritte verbessert werden müssen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_measurements = {}\n",
        "\n",
        "def start_time_measurement(eventName):\n",
        "    \"\"\"\n",
        "    Use this function to start a time measurement for a specific event.\n",
        "    A end_time_measurement() call with the same name must be called before the next start_time_measurement() call.\n",
        "    \"\"\"\n",
        "    add = False\n",
        "    if eventName not in time_measurements:\n",
        "        time_measurements[eventName] = {\n",
        "            \"name\": eventName,\n",
        "            \"start\": [],\n",
        "            \"end\": [],\n",
        "            \"count\": 0\n",
        "        }\n",
        "        add = True\n",
        "    elif len(time_measurements[eventName][\"start\"]) > len(time_measurements[eventName][\"end\"]):\n",
        "        print(f\"Time measure error: Event '{eventName}' not finished before reassignment!\")\n",
        "    else:\n",
        "       add = True\n",
        "    \n",
        "    #start measurement as late as possible\n",
        "    if add:\n",
        "        time_measurements[eventName][\"start\"].append(time.perf_counter_ns())\n",
        "\n",
        "def end_time_measurement(eventName):\n",
        "    \"\"\"\n",
        "    Use this function to end a time measurement for a specific event.\n",
        "    A time measurement with the same name must be started with start_time_measurement() before it can be ended.\n",
        "    \"\"\"\n",
        "    #end measurement as fast as possible\n",
        "    temp_time = time.perf_counter_ns()\n",
        "    if eventName not in time_measurements:\n",
        "        print(f\"Time measure error: Event '{eventName}' not defined!\")\n",
        "    elif len(time_measurements[eventName][\"end\"]) >= len(time_measurements[eventName][\"start\"]):\n",
        "        print(f\"Time measure error: Event '{eventName}' not started before reassignment!\")\n",
        "    else:\n",
        "        time_measurements[eventName][\"end\"].append(temp_time)\n",
        "        time_measurements[eventName][\"count\"] += 1\n",
        "    \n",
        "def analyse_time_measurements():\n",
        "    \"\"\"\n",
        "    Analyse time measurements and print them in a table.\n",
        "    Multiple events with the same name are averaged.\n",
        "    \"\"\"\n",
        "    time_measurements_table = PrettyTable([\"Name\", \"Avg. [ms]\", \"Min. [ms]\", \"Max. [ms]\", \"Occurrences [compl.]\"])\n",
        "    time_measurements_table.align[\"Name\"] = \"l\"\n",
        "    time_measurements_table.align[\"Avg. [ms]\"] = \"r\"\n",
        "    time_measurements_table.align[\"Min. [ms]\"] = \"r\"\n",
        "    time_measurements_table.align[\"Max. [ms]\"] = \"r\"\n",
        "    time_measurements_table.align[\"Occurrences [compl.]\"] = \"r\"\n",
        "    for key, event in time_measurements.items():\n",
        "        timings = []\n",
        "        if len(event[\"start\"]) != len(event[\"end\"]):\n",
        "            print(f\"Time measure error: Event '{key}' has different amounts of values for start and end times!\")\n",
        "            continue\n",
        "        \n",
        "        for i in range(len(event[\"start\"])):\n",
        "            timing = (event[\"end\"][i] - event[\"start\"][i])\n",
        "            #exclude 0 values\n",
        "            if timing >= 0:\n",
        "                timing = timing / (1000 * 1000) #convert from ns to ms\n",
        "                timings.append(timing)\n",
        "\n",
        "        event[\"min\"] = min(timings) \n",
        "        event[\"max\"] = max(timings)\n",
        "        event[\"avg\"] = sum(timings) / len(event[\"start\"])\n",
        "\n",
        "        time_measurements_table.add_row([key, '{0:.2f}'.format(event[\"avg\"]), '{0:.2f}'.format(event[\"min\"]), '{0:.2f}'.format(event[\"max\"]), event[\"count\"]])\n",
        "        \n",
        "    print(time_measurements_table)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kamerakalibrierung\n",
        "\n",
        "Die Abbildung eines 3D-Objekts auf eine 2D-Bildebene mit einer Kamera wird durch die internen Größen einer Kamera beeinflusst. Dazu gehören zum Beispiel die Bildmitte, die Brennweite und auch Kameraverzerrungsparameter.\n",
        "\n",
        "Mit Hilfe der Kamerakalibrierung werden die sogenannten \"intrinsische\" und \"extrinsische\" Parameter berechnet. Intrinsische Parameter beschreiben die Kalibriermatrix (Kamera zu Pixel), während die extrinsischen Parameter die Beziehung zwischen dem Koordinatensystem der Kamera und dem Welt-Koordinatensystem beschreiben (Welt zu Kamera).\n",
        "\n",
        "Zusammen bilden diese Parameter eine Projektionsmatrix, die es ermöglicht, gemachte Bilder im Welt-Koordinatensystem darzustellen (die Bilder werden entzerrt). \n",
        "\n",
        "In folgendem Abschnitt wird die Kamera mit Hilfe von Schachbrett-Bildern kalibriert. Diese Aufgabe mit den Ergebnissen auf den Schachbrettbildern befinden sich im JupyterNotebook [Projekt_Spurerkennung_v2.jpynb](Projekt_Spurerkennung_v2.ipynb).\n",
        "\n",
        "\n",
        "Mit den unten definierten Funktionen (siehe ```undistort_image``` und ```undistort_image_remap```) werden dann die Frames der Videos anhand der berechneten Transformations-Matrix entzerrt, um eine tatsächliche Darstellung im Welt-Koordinatensystem zu erhalten. Es wurden zwei Funktionen implementiert:\n",
        "\n",
        "- ```undistort_image```, wie sie in [Projekt_Spurerkennung_v2.jpynb](Projekt_Spurerkennung_v2.ipynb) gezeigt wurde, berechnet für jedes mitgegebene Frame die passenden Parameter und entzerrt das Bild dementsprechend.\n",
        "- ```undistort_image_remap``` berechnet eine Matrix anhand des ersten übergebenen Frames und wendet diese Matrix bei weiteren Aufrufen auf die jeweils übergebenen Frames an.\n",
        "\n",
        "### <p style = \"color: red\"> Was genau machen die Funktionen jeweils -> Wir können eig nicht 50% schneller sein</p>\n",
        "### <p style = \"color: red\"> Müssen wir nochmal Benchmarken, die Funktionen machen genau das gleiche, siehe folgende Links</p>\n",
        "### <p style = \"color: red\"> undistort_image: https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga69f2545a8b62a6b0fc2ee060dc30559d</p>\n",
        "### <p style = \"color: red\"> undistort_image_remap: https://docs.opencv.org/3.4/da/d54/group__imgproc__transform.html#ga7dfb72c9cf9780a347fbe3d1c47e5d5a</p>\n",
        "\n",
        "Das führt dazu, dass mit ```undistort_image_remap``` eine ungefähr 75% performantere Lösung für die Kamerakalibrierung gefunden wurde (vgl. Abbildung 2).\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"img/documentation/calibrationFaster.png\" width=\"15%\" height=\"15%\" />\n",
        "  <br>\n",
        "  <em>Abbildung 3: Dauer jeder Funktion für jeweils 3 Durchläufe</em>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# size of chessboard, minimum error with (7, 6), but there were severe artifacts at the borders (see error calculation at the end)\n",
        "chessboard_x, chessboard_y = 9, 6\n",
        "\n",
        "# termination criteria\n",
        "criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
        "obj_points = np.zeros((chessboard_y * chessboard_x, 3), np.float32)\n",
        "obj_points[:, :2] = np.mgrid[0:chessboard_x, 0:chessboard_y].T.reshape(-1, 2)\n",
        "# Arrays to store object points and image points from all the images.\n",
        "object_points = []  # 3d point in real world space\n",
        "image_points = []  # 2d points in image plane.\n",
        "\n",
        "# use all calibration images\n",
        "images = glob.glob(\"./img/Udacity/calib/*.jpg\")\n",
        "for i, frame_name in enumerate(images):\n",
        "    image = cv.imread(frame_name)\n",
        "    image_gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Find the chess board corners\n",
        "    ret, corners = cv.findChessboardCorners(image_gray, (chessboard_x, chessboard_y), None)\n",
        "    # If found, add object points, image points (after refining them)\n",
        "    if ret == True:\n",
        "        object_points.append(obj_points)\n",
        "        corners2 = cv.cornerSubPix(\n",
        "            image_gray, corners, (11, 11), (-1, -1), criteria\n",
        "        )  # improve accuracy of corners\n",
        "        image_points.append(corners)\n",
        "\n",
        "ret, camera_matrix, dist_coefficient, rvecs, tvecs = cv.calibrateCamera(\n",
        "    object_points, image_points, image_gray.shape[::-1], None, None\n",
        ")\n",
        "\n",
        "image_height, image_width = image.shape[:2]\n",
        "new_camera_matrix, roi = cv.getOptimalNewCameraMatrix(camera_matrix, dist_coefficient, (image_width, image_height), 1, (image_width, image_height))\n",
        "\n",
        "map_x, map_y = cv.initUndistortRectifyMap(camera_matrix, dist_coefficient, None, new_camera_matrix, (image_width, image_height), 5)\n",
        "\n",
        "# ------- define functions for image processing -------\n",
        "@deprecated\n",
        "def undistort_image(img):\n",
        "    img_undistorted = cv.undistort(img, camera_matrix, dist_coefficient, None, new_camera_matrix)\n",
        "    # crop the image\n",
        "    x, y, image_width, image_height = roi\n",
        "    return img_undistorted[y : y + image_height, x : x + image_width]\n",
        "\n",
        "# ~75% faster than undistort_image()\n",
        "def undistort_image_remap(img):\n",
        "    \"\"\"Undistort image using remap function without recalculating the map every time.\"\"\"\n",
        "    dst = cv.remap(img, map_x, map_y, cv.INTER_LINEAR)\n",
        "    # crop the image\n",
        "    x, y, image_width, image_height = roi\n",
        "    return dst[y : y + image_height, x : x + image_width]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Perspektivtransformation\n",
        "\n",
        "Die Perspektivtransformation wird verwendet, um die \"Region of Interest\" festzulegen. Dadurch muss nicht mehr das gesamte Bild verarbeitet werden, sondern es kann nur ein der Bereich, in dem sich die Fahrspur befindet, zur Verarbeitung genutzt werden.\n",
        "\n",
        "Hierbei werden vier feste Punkte im originalen Bild ausgewählt. Diese Punkte werden dann in einem Bild mit den selben Dimensionen wie das Originalbild dargestellt, um eine rechteckige Beziehung zwischen den Punkten herzustellen (siehe Abbildung 3). Die Transformation geschieht mit der Funktion ```warp_image_udacity```.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"img/documentation/perspectiveTransformation.png\" />\n",
        "  <br>\n",
        "  <em>Abbildung 3: Beispiel für implementierte Perspektivtransformation</em>\n",
        "</p>\n",
        "\n",
        "Um die mit einer später erklärten Funktion (```sliding_windows```) in dem transformierten Bild gefundenen Punkte in dem originalen Bild anzeigen zu können, müssen diese Punkte zurücktransformiert werden. Um Rechenleistung zu sparen, werden tatsächlich nur die Punkte transformiert (siehe ```rewarp_points_udacity```), bei einem ersten Ansatz wurde das gesamte Bild zurücktransformiert (siehe ```rewarp_image_udacity```). \n",
        "\n",
        "Eine weitere Überlegung war es, das Bild nach der Perspektivtransformation zu verkleinern. Damit würde die Anzahl an zu verarbeitenden Pixeln deutlich sinken, was eine Verbesserung der FPS mit sich bringen würde. Jedoch traten Probleme bei der Rücktransformation der Punkte des kleinen Bildes in die des Originalframes auf, weshalb diese Idee nicht weiter verfolgt wurde."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# udacity images\n",
        "src_udacity = np.float32([[191, 628], [531, 404], [1021, 628], [681, 404]])\n",
        "dst_udacity = np.float32([[150, 720], [150, 10], [1000, 720], [1000, 10]])\n",
        "\n",
        "M_warp = cv.getPerspectiveTransform(src_udacity, dst_udacity)\n",
        "M_rewarp = cv.getPerspectiveTransform(dst_udacity, src_udacity)\n",
        "\n",
        "\n",
        "def warp_image_udacity(img):\n",
        "    \"\"\"Warp image to bird's eye view. Uses fix transformation matrix.\n",
        "\n",
        "    Args:\n",
        "        img (_type_): input image\n",
        "\n",
        "    Returns:\n",
        "        _type_: image in bird's eye view\n",
        "    \"\"\"\n",
        "    image = cv.warpPerspective(img, M_warp, (img.shape[1], img.shape[0]))\n",
        "    if DEBUG_MODE:\n",
        "        #Draw a red circle with zero radius and -1 for filled circle\n",
        "        for src in src_udacity:\n",
        "            image2 = cv.circle(img, np.int32(src), radius=0, color=(0, 0, 255), thickness=5)\n",
        "        cv.imshow(\"Transformation points\", image2)\n",
        "    return image\n",
        "\n",
        "@deprecated\n",
        "def rewarp_image_udacity(img):\n",
        "    img = cv.warpPerspective(img, M_rewarp, (img.shape[1], img.shape[0]), cv.WARP_INVERSE_MAP)\n",
        "    return img\n",
        "\n",
        "def rewarp_points_udacity(points):\n",
        "    \"\"\"\n",
        "    Rewarp points from warped image coordinates to original image coordinates.\n",
        "    \n",
        "    Args:\n",
        "        points (np.array): float32 array \n",
        "        \n",
        "    @return: points in original image coordinates\n",
        "    \"\"\"\n",
        "    return cv.perspectiveTransform(points, M_rewarp)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \"Sliding Windows\"\n",
        "\n",
        "Die Erkennung der Fahrbahnmarkierungen wird mit den sogenannten \"Sliding Windows\" umgesetzt. \n",
        "\n",
        "### <p style=\"color: red\"> Es passiert genau das was ich dachte -> Genauer beschreiben: Histogram über ganzes Bild, Peaks von weißen Pixeln auf X-Koordinaten, Splitten des Bild in zwei Hälften -> 2 Sliding-Windows, Höhe der Windows anhand der Anzahl berechnet </p>\n",
        "\n",
        "Kurz gefasst sind \"Sliding Windows\" kleine Kästchen, die anhand der Helligkeit von Pixel ihren Mittelpunkt anpassen. Damit kann beispielsweise eine weiße Linie auf schwarzem Hintergrund fast perfekt verfolgt werden. \n",
        "\n",
        "Detailreicher beginnt die ```sliding_windows``` Funktion damit, anhand des Histogramms eines Frames die initialen Mittelpunkte der ersten Windows festzulegen. Anhand dieser Mittelpunkte werden eine angegebene Anzahl an Windows erzeugt, die aufeinander gestapelt werden. Das bedeutet, der X-Wert der Windows bleibt gleich, der Y-Wert passt sich an.\n",
        "\n",
        "Für jedes Window wird nun geprüft, wie viele hellen Pixel von dem jeweiligen Window eingegrenzt werden. Wenn der Wert der gefundenen Pixel das angegebene Minimum unterschreitet, wird der Mittelpunkt des jeweiligen Windows angepasst.\n",
        "\n",
        "Ein Beispiel für diese \"Sliding Windows\" zeigt Abbildung 4.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"img/documentation/slidingWindows.png\" width=\"40%\" height=\"40%\"/>\n",
        "  <br>\n",
        "  <em>Abbildung 4: Beispiel für die Darstellung der \"Sliding Windows\"</em>\n",
        "</p>\n",
        "\n",
        "Damit kann die Fahrspur bei einer Veränderung leicht weiterverfolgt und markiert werden. Die hier gefundenen Mittelpunkte bilden die Grundlage für die Polynomberechnung.\n",
        "\n",
        "Eine Alternative zu \"Sliding Windows\" wäre, die Spur mit Hilfe einer Kantenerkennung durchzuführen. Hierbei entsteht aber das Problem, dass selbst kleine Kanten wie die bei einem Asphaltwechsel einen enormen Störfaktor darstellen. \"Sliding Windows\" sind dementsprechend einfach präziser und optimieren die Erkennung deutlich, weshalb wir uns für eine Umsetzung damit entschieden haben."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "number_windows = 50\n",
        "margin = 50\n",
        "def sliding_windows(frame, window_width=200, minimum_whites=30):\n",
        "    \"\"\"Divides frame into multiple lines of equal height.\n",
        "    For every frame the peaks of the histogram for a right and a left side are evaluated.\n",
        "    One window is placed on both sides with the peak point as middle point.\n",
        "    If there are more white pixels than specified with minimum_whites inside the specified boundaries of the box, its middle point is stored as as\n",
        "    good value inside the ..._goods arrays. (separated for right and left windows for each line).\n",
        "    If the average of the pixels are not in the center of the sliding window, the new preset center for the next sliding window is set to\n",
        "    the point of highest density inside the current window.\n",
        "\n",
        "    Args:\n",
        "        frame (image): input frame with masked lane lines\n",
        "        window_width (int, optional): width of windows. Defaults to 200.\n",
        "        minimum_whites (int, optional): minimum of white pixel per sliding window to in order to be marked as valid. Defaults to 30.\n",
        "\n",
        "    Returns:\n",
        "        lefts, rights (np.array): returns left and right points as 2d array of sliding window centers\n",
        "    \"\"\"\n",
        "    # Histogram for image\n",
        "    hist = np.sum(frame[frame.shape[0]//2:, :], axis=0)\n",
        "        \n",
        "    # Take peaks from left and right side of histogram for starting points and add half margin\n",
        "    mid_point_x = np.int32(hist.shape[0] // 2)\n",
        "    left_x_start = np.argmax(hist[:mid_point_x]) - window_width // 2\n",
        "    right_x_start = np.argmax(hist[mid_point_x:]) + mid_point_x + window_width // 2\n",
        "    # Window height based on number of windows\n",
        "    window_height = np.int32(frame.shape[0] // number_windows)\n",
        "    \n",
        "    # Calc points that are not zero in images\n",
        "    nonzero = frame.nonzero()\n",
        "    nonzero_y = np.array(nonzero[0])\n",
        "    nonzero_x = np.array(nonzero[1])\n",
        "    \n",
        "    # Initialize current positions for windows\n",
        "    left_x_current = left_x_start\n",
        "    right_x_current = right_x_start\n",
        "\n",
        "    # Initialize values to be returned -> centers of windows\n",
        "    lefts_good = np.empty((0,2), dtype=np.int32)\n",
        "    rights_good = np.empty((0,2), dtype=np.int32)\n",
        "\n",
        "    # Go through every window\n",
        "    for window in range(number_windows):\n",
        "        # Identify window boundaries in x and y (and right and left)\n",
        "        win_y_low = frame.shape[0] - (window + 1) * window_height\n",
        "        win_y_high = frame.shape[0] - window*window_height\n",
        "        y_mid = (win_y_low + win_y_high) // 2\n",
        "        \n",
        "        # Calculate boundaries of the window\n",
        "        win_xleft_low = left_x_current - window_width  \n",
        "        win_xleft_high = left_x_current + window_width  \n",
        "        win_xright_low =  right_x_current - window_width \n",
        "        win_xright_high = right_x_current + window_width  \n",
        "        \n",
        "        # Identify the pixels that are not zero within window\n",
        "        left_inds = ((nonzero_y >= win_y_low ) & (nonzero_y < win_y_high) & (nonzero_x >= win_xleft_low) & (nonzero_x < win_xleft_high)).nonzero()[0]\n",
        "        right_inds = ((nonzero_y >= win_y_low ) & (nonzero_y < win_y_high) & (nonzero_x >= win_xright_low) & (nonzero_x < win_xright_high)).nonzero()[0]\n",
        "        \n",
        "        # If more than minimum pixels are found -> recenter next window\n",
        "        if len(left_inds) > minimum_whites:\n",
        "            left_x_current = np.int32(np.mean(nonzero_x[left_inds]))\n",
        "            lefts_good = np.concatenate((lefts_good, [[left_x_current, y_mid]]))\n",
        "            if DEBUG_MODE:\n",
        "                cv.rectangle(frame, (left_x_current - margin, win_y_low),(left_x_current + margin, win_y_high),(255, 255, 255), 2)\n",
        "        if len(right_inds) > minimum_whites:\n",
        "            right_x_current = np.int32(np.mean(nonzero_x[right_inds]))\n",
        "            rights_good = np.concatenate((rights_good, [[right_x_current, y_mid]]))\n",
        "            if DEBUG_MODE:\n",
        "                cv.rectangle(frame, (right_x_current - margin, win_y_low),(right_x_current + margin, win_y_high),(255, 255, 255), 2)\n",
        "\n",
        "    return lefts_good, rights_good"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Polynome\n",
        "\n",
        "Um eine Veränderung der Fahrspur, z.B. bei Kurven, angemessen darstellen zu können, werden anhand der von ```sliding_windows``` gefundenen Punkte Polynome berechnet.\n",
        "\n",
        "Um zu verhindern, dass sich die Polynome durch ungültige oder zu wenig Punkte nicht ordentlich an die Fahrspur anpassen, werden die unten beschriebenen Abfragen ausgeführt.\n",
        "\n",
        "1. Wenn die Anzahl der an die Funktion ```calculate_polynomial_points``` übergebenen Punkte pro Seite (rechts und links) kleiner ist als zwei, so werden keine Polynome berechnet, da diese sonst zu ungenau sind. Das bedeutet, es werden auch keine Punkte anhand von Polynomen berechnet, es wird also \"None\" als Rückgabewert festgelegt. Was das für die Spurerkennung bedeutet, wird im nächsten Abschnitt beschrieben.\n",
        "\n",
        "2. Sind genug Punkte zur Berechnung eines Polynoms vorhanden, werden diese berechnet. Daraufhin wird geprüft, ob der Parameter des höchsten Grades der Polynome einen vordefinierten Wert überschreitet. Wenn das der Fall ist, enthalten die gegebene Punkte zur Polynomberechnung ungültige Werte. Das führt zu einer extremen, unrealistischen Krümmung der erkannten Fahrspur. Deshalb werden auch hier die berechneten Punkte nicht zur weiteren Verarbeitung zurückgegeben.\n",
        "\n",
        "3. Sollten sich die anhand der Polynome berechneten Punkte zu nahe sein, ist mit hoher Wahrscheinlichkeit ein Schnittpunkt vorhanden. Da auch dieses Szenario in keinem der bereitgestellten Videos vorkommt, werden auch hier keine Punkte zurückgegeben. Falls die Distanz zwischen der berechneten Punkte zu groß wird, werden diese Punkte auch gestrichen. Das kann damit begründet werden, dass sich die Fahrspur in keinem der gegebene Videos plötzlich verbreitert.\n",
        "\n",
        "Sollten die Punkte jedoch vollständig gültig sein bilden diese die rechten und linken Grenzen der einzufärbenden Fläche. Dadurch lassen sich selbst kleine Änderungend der Fahrspur ordentlich darstellen (siehe Abbildung 5).\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"img/documentation/polynoms.png\" width=\"40%\" height=\"40%\"/>\n",
        "  <br>\n",
        "  <em>Abbildung 5: Darstellung der berechneten Polynome auf der Fahrspur</em>\n",
        "</p>\n",
        "\n",
        "Zum Einzeichnen der gesamten Fahrspur werden die anhand der berechneten Polynome ausgerechneten Punkte an die nächste Funktion ```drawRecOnFrame``` weitergegeben."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  calculate polynomial from rewarped points\n",
        "max_polynom_curvature_value = 0.005 # defines how much the polynom can be curved before it is considered faulty\n",
        "\n",
        "def calculate_polynomial_points(lefts, rights):\n",
        "    \"\"\"\n",
        "    calculate polynomial from rewarped points and then return list of points on the polynomial\n",
        "    @return: array of points for left and right polynomial\n",
        "    \"\"\"\n",
        "    # flags for faulty polynom sides\n",
        "    is_left_faulty, is_right_faulty = False, False\n",
        "    \n",
        "    # check if there are enough points\n",
        "    if len(lefts) < 2 or len(rights) < 2:\n",
        "        return None, None\n",
        "    \n",
        "    # calculate polynomial from rewarped points\n",
        "    left_polynom_values = np.polyfit(lefts[:,1], lefts[:,0], 2)\n",
        "    right_polynom_values = np.polyfit(rights[:,1], rights[:,0], 2)\n",
        "\n",
        "    # check if polynom is valid (not too steeply curved)\n",
        "    if (left_polynom_values[0] > max_polynom_curvature_value or left_polynom_values[0] < -max_polynom_curvature_value):\n",
        "        is_left_faulty = True\n",
        "    if (right_polynom_values[0] > max_polynom_curvature_value or right_polynom_values[0] < -max_polynom_curvature_value):\n",
        "        is_right_faulty = True\n",
        "    if is_left_faulty and is_right_faulty:\n",
        "        return None, None\n",
        "    \n",
        "    # 750 as x length of polynom\n",
        "    x_axis = np.linspace(0, 750, 750)\n",
        "\n",
        "    # calculate y values for left and right line\n",
        "    left_line_y = left_polynom_values[0]*x_axis**2 + left_polynom_values[1]*x_axis + left_polynom_values[2]\n",
        "    right_line_y = right_polynom_values[0]*x_axis**2 + right_polynom_values[1]*x_axis + right_polynom_values[2]\n",
        "\n",
        "    # array of points for left and right line from x and y values\n",
        "    left_pts = np.array([np.transpose(np.vstack([left_line_y, x_axis]))])\n",
        "    right_pts = np.array([np.transpose(np.vstack([right_line_y, x_axis]))])\n",
        "    \n",
        "    # loop through all points and check if they are to close or to far away from each other\n",
        "    # (if they are to close -> most likely a interception point)\n",
        "    for i in range(len(left_pts[0])):\n",
        "        y_distance = abs(left_pts[0][i][0] - right_pts[0][i][0])\n",
        "        if y_distance < 10 or y_distance > 1000:\n",
        "            return None, None\n",
        "\n",
        "    # only return non faulty polynom points\n",
        "    return left_pts if not is_left_faulty else None, right_pts if not is_right_faulty else None"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Enzeichen der Fahrspur\n",
        "\n",
        "Mit Hilfe der Funktion ```drawRecOnFrame``` wird die Fläche zwischen den Punkten, die durch die Polynome berechnet wurden, eingezeichnet (siehe Abbildung 6).\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"img/documentation/filledLane.png\" width=\"40%\" height=\"40%\"/>\n",
        "  <br>\n",
        "  <em>Abbildung 6: Markierung der Fahrbahn mit Polynomen als Grenzen</em>\n",
        "</p>\n",
        "\n",
        "Falls, wie oben beschrieben, nicht genug Punkte vorhanden sind, um die Fläche einzuzeichnen, bleibt die zuletzt eingezeichnete Fläche einfach vorhanden. In diesem Fall beschränken wir die minimale Gesamtanzahl an Punkten, die vorhanden sein müssen um die Fläche einzuzeichnen, auf vier. Das hängt mit der im vorherigen Abschnitt beschriebenen Grenze zur Berechnung der Polynome zusammen.\n",
        "\n",
        "Zum aktuellen Zeitpunkt existiert kein Szenario, in dem die Spur nicht genug erkannt wird, um diese Funktion auszulösen. Falls die Spurerkennung im Video \"harder_challenge_video\" durchgeführt wird, lässt sich diese Funktion aber gut beobachten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def drawRecOnFrame(frame, left_pts, right_pts):\n",
        "    \"\"\"\n",
        "    Rewarp points to original image coordinates and draw rectangle on frame.\n",
        "    \n",
        "    input: array of points from left and right lane marking in warped image\n",
        "    e.g.:\n",
        "      [[ 281   39]\n",
        "      [ 971  163]\n",
        "      [ 958  101]]\n",
        "    \"\"\"\n",
        "    if len(left_pts) + len(right_pts) > 3:\n",
        "      borderPoints = np.concatenate((np.flip(left_pts, axis=0), right_pts))\n",
        "      borderPointsRewarped = rewarp_points_udacity(np.array([borderPoints], dtype=np.float32))\n",
        "      borderPointsRewarpedInt = borderPointsRewarped.astype(int)\n",
        "      # draw final polygon in frame\n",
        "      cv.drawContours(frame, borderPointsRewarpedInt, -1, (0,255,0), -1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Maskierung\n",
        "\n",
        "Mit ```applyMasks``` werden bestimmte Farbmasken, genauer eine für Weiß und eine für Gelb, auf das Bild gelegt. Damit werden nur diese zwei Farben in den Bildern hervorgehoben bzw. dargestellt. \n",
        "\n",
        "Diese Funktion ist inzwischen veraltet, da die Funktion ```lane_detection``` eine verbesserte Implementierung mit neuen Filtern zur Verfügung stellt.\n",
        "\n",
        "In der Funktion ```lane_detection``` werden zwar weiterhin Farbfilter angewandt, um nur die für die Spurerkennung wichtigen Farben hervorzuheben, dazu werden aber auch noch morphologische Filter auf den Frame im HLS-Farbraum angewandt.\n",
        "Dazu gehört zum Beispiel \"Opening\", um einzelne Teile der Spur von anderen zu trennen und somit leichter erkennbar zu machen. \n",
        "\n",
        "Zusammengefasst lässt sich sagen, dass mit Hilfe der Funktion ```lane_detection``` für jede relevante Farbe (Weiß und Gelb) eine verbesserte Hervorhebung durch Masken und Filter entsteht. Dadurch können die Fahrbahnmarkierungen mit Hilfe der Sliding-Windows deutlich robuster verfolgen bzw. erkennen, was auch der Grund für einen Wechsel zur neuen Funktion darstellt.\n",
        "\n",
        "### <p style=\"color: red\"> Genauer? </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@deprecated\n",
        "def applyMasks(frame):\n",
        "    \"\"\"\n",
        "    Apply masks to frame and return frame with only lane marking.\n",
        "    \"\"\"\n",
        "    ## convert to hsv\n",
        "    hls_frame = cv.cvtColor(frame, cv.COLOR_BGR2HLS)\n",
        "\n",
        "    ## mask for white\n",
        "    white_mask = cv.inRange(hls_frame, (0, 200, 0), (255, 255,255))\n",
        "\n",
        "    ## mask for yellow\n",
        "    # yellow_mask = cv.inRange(hls_frame, (20,90,200), (26, 255, 255))\n",
        "    yellow_mask = cv.inRange(hls_frame, (10,0,100), (40, 255, 255))\n",
        "\n",
        "    ## final mask and masked\n",
        "    mask = cv.bitwise_or(white_mask, yellow_mask)\n",
        "    frame = cv.bitwise_and(frame,frame, mask=mask)\n",
        "\n",
        "    return frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lane_detection(frame, white_lower, white_upper, yellow_lower, yellow_upper):\n",
        "    \"\"\" Use color masks to detect lane markings in bird's eye view. \n",
        "    \n",
        "    Uses Gaussian blur and HSV color space, then applies masks for white and yellow lane markings. Applies morphological operations to remove noise and fill gaps. Then combines masks and applies them to the input frame.\n",
        "\n",
        "    Args:\n",
        "        frame (_type_): input frame to mask lane markings in bird's eye view\n",
        "        white_lower (_type_): lower bound for white mask\n",
        "        white_upper (_type_): upper bound for white mask\n",
        "        yellow_lower (_type_): lower bound for yellow mask\n",
        "        yellow_upper (_type_): upper bound for yellow mask\n",
        "\n",
        "    Returns:\n",
        "        image: masked image with lane markings\n",
        "    \"\"\"\n",
        "    if DEBUG_MODE: # needs frame at this point\n",
        "        greenImg = np.zeros(frame.shape, frame.dtype)\n",
        "        redImg = np.zeros(frame.shape, frame.dtype)\n",
        "        \n",
        "    frame = cv.GaussianBlur(frame, (5, 5), 0)\n",
        "    frame_hls = cv.cvtColor(frame, cv.COLOR_BGR2HLS) #-> Weiß\n",
        "    frame_lab = cv.cvtColor(frame, cv.COLOR_BGR2LAB) # -> Gelb\n",
        "\n",
        "    white_mask = cv.inRange(frame_hls, white_lower, white_upper)\n",
        "    # white_mask[:, 0:200] = 0 # could be used for harder challenge video to ignore left side of image for white mask\n",
        "\n",
        "    yellow_mask = cv.inRange(frame_lab, yellow_lower, yellow_upper)\n",
        "    # yellow_mask[:, 1000:] = 0 # could be used for harder challenge video to ignore right side of image for yellow mask\n",
        "\n",
        "    combined_mask = cv.bitwise_or(white_mask, yellow_mask)\n",
        "    \n",
        "    kernel = cv.getStructuringElement(cv.MORPH_RECT, (13, 13))\n",
        "    lanes_yellow = cv.morphologyEx(frame_lab[:, :, 2], cv.MORPH_TOPHAT, kernel) # -> kleine helle sachen in dunkler umgebung hervorheben (hebt Gelb deutlich hervor im LAB-Farbraum)\n",
        "    ret, lanes_yellow = cv.threshold(lanes_yellow, thresh=2, maxval=255, type=cv.THRESH_BINARY) \n",
        "    lanes_yellow = cv.morphologyEx(lanes_yellow, cv.MORPH_OPEN, np.array([[0,1,0],[1,1,1],[0,1,0]], 'uint8'), iterations=4)\n",
        "\n",
        "    combined_mask = cv.bitwise_or(combined_mask, lanes_yellow) # -> Lanes_yellow = verbesserter Yellowfilter\n",
        "    combined_mask = cv.morphologyEx(combined_mask, cv.MORPH_OPEN, np.array([[0,1,0],[1,1,1],[0,1,0]], 'uint8'), iterations=2)\n",
        "\n",
        "    frame = cv.bitwise_and(frame, frame, mask=combined_mask)\n",
        "    frame = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "    if DEBUG_MODE: # show combined debug images\n",
        "        redImg[:,:] = (0, 0, 255)\n",
        "        redMask = cv.bitwise_and(redImg, redImg, mask=yellow_mask)\n",
        "\n",
        "        greenImg[:,:] = (0, 255, 0)\n",
        "        greenMask = cv.bitwise_and(greenImg, greenImg, mask=white_mask)\n",
        "\n",
        "        frame_mask = redMask + greenMask\n",
        "\n",
        "        cv.imshow(\"Current Mask\", frame_mask)\n",
        "        cv.imshow(\"Combined Mask\", combined_mask)\n",
        "\n",
        "\n",
        "    return frame"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Helligkeitskorrektur\n",
        "\n",
        "Damit die Fahrbahnmarkierungen in dunklen sowie in sehr hellen Elementen der Videos zu erkennen sind, wurde eine dynamische Helligkeitsanpassung als Zusatzfunktion implementiert.\n",
        "\n",
        "Der Frame wird in den HSV-Farbraum umgewandelt und in die jeweiligen Bestandteile aufgeteilt. Daraufhin wird der durchschnittliche Helligkeitswert des Frames berechnet.\n",
        "\n",
        "Wenn dieser Helligkeitswert dann kleiner oder größer als die angegebenen Grenzen ist, so wird die Helligkeit des Frames angepasst.\n",
        "\n",
        "Bei sehr hellen Bereichen wird die durchschnittliche Hellligkeit soweit nach unten korrigiert, dass sie sich dann zwischen den gesetzten Grenzen befindet (vgl. Abbildung 7). \n",
        "Selbes gilt für dunkle Bereiche.\n",
        "\n",
        "<p align=\"middle\" float=\"left\">\n",
        "  <img src=\"img/documentation/brightnessCorrectionNormal.png\" width=\"40%\" height=\"40%\"/>\n",
        "  <img src=\"img/documentation/brightnessCorrectionCorrected.png\" width=\"40%\" height=\"40%\"/>\n",
        "  <br>\n",
        "  <em>Abbildung 7: Helligkeitskorrektur - Links: Original, Rechts: Korrigiert</em>\n",
        "</p>\n",
        "\n",
        "Damit werden Teile der Fahrbahnmarkierungen, die davor nicht von den festgelegten Masken und Filter erkannt werden konnten, so angepasst, dass sie im gefilterten Bild zu erkennen sind. Das macht die Fahrspurerkennung deutlich robuster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def correct_Brightness(image):\n",
        "    \"\"\"Calculate the brightness of the overall image and adjust it if it is too bright or too dark.\n",
        "\n",
        "    Args:\n",
        "        image: input image\n",
        "\n",
        "    Returns:\n",
        "        image, brightness: corrected image and new overall brightness\n",
        "    \"\"\"\n",
        "    image_hsv = cv.cvtColor(image, cv.COLOR_BGR2HSV)\n",
        "    image_brightness_input = image_hsv[...,2].mean()\n",
        "    brightness_upper_limit = 150\n",
        "    brightness_lower_limit = 85\n",
        "    if image_brightness_input > brightness_upper_limit:\n",
        "        h, s, v = cv.split(image_hsv)\n",
        "        \n",
        "        lim = round(0 + image_brightness_input - brightness_upper_limit)\n",
        "        v[v < lim] = 0\n",
        "        v[v >= lim] -= lim\n",
        "\n",
        "        final_hsv = cv.merge((h, s, v))\n",
        "        image = cv.cvtColor(final_hsv, cv.COLOR_HSV2BGR)\n",
        "    \n",
        "    elif image_brightness_input < brightness_lower_limit:\n",
        "        h, s, v = cv.split(image_hsv)\n",
        "        \n",
        "        lim = round(255 + brightness_lower_limit - image_brightness_input)\n",
        "        v[v > lim] = 255\n",
        "        v[v <= lim] += lim\n",
        "\n",
        "        final_hsv = cv.merge((h, s, v))\n",
        "        image = cv.cvtColor(final_hsv, cv.COLOR_HSV2BGR)\n",
        "\n",
        "    image_hsv = cv.cvtColor(image, cv.COLOR_BGR2HSV)\n",
        "    image_brightness_new = image_hsv[...,2].mean()\n",
        "\n",
        "    return image, image_brightness_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def correct_Brightness_and_Split(image):\n",
        "    # ----------- Split in quadrants ---------------\n",
        "    start_time_measurement(\"quadrant splitting\")\n",
        "    height_cutoff = image.shape[0] // 2\n",
        "    width_cutoff = image.shape[1] // 2\n",
        "\n",
        "    left_side = image[:, :width_cutoff]\n",
        "    right_side = image[:, width_cutoff:]\n",
        "\n",
        "    l1 = left_side[:height_cutoff, :]\n",
        "    l2 = left_side[height_cutoff:, :]\n",
        "\n",
        "    r1 = right_side[:height_cutoff, :]\n",
        "    r2 = right_side[height_cutoff:, :]\n",
        "    end_time_measurement(\"quadrant splitting\")\n",
        "    \n",
        "    # ----------- Correct brightness ---------------\n",
        "    start_time_measurement(\"brightness correction\")\n",
        "    l2, brightness_l2 = correct_Brightness(l2)\n",
        "    l1, brightness_l1 = correct_Brightness(l1)\n",
        "    r2, brightness_r2 = correct_Brightness(r2)\n",
        "    r1, brightness_r1 = correct_Brightness(r1)\n",
        "    end_time_measurement(\"brightness correction\")\n",
        "    \n",
        "    # ----------- Merge the quadrants ---------------\n",
        "    start_time_measurement(\"quadrant merging\")\n",
        "    numpy_vertical_l = np.vstack((l1, l2))\n",
        "    numpy_vertical_r = np.vstack((r1, r2))\n",
        "    grayscale_frame = np.hstack((numpy_vertical_l, numpy_vertical_r))\n",
        "    end_time_measurement(\"quadrant merging\")\n",
        "    \n",
        "    if DEBUG_MODE:\n",
        "        # draw borders around the quadrants and display the brightness\n",
        "        l2_border = cv.copyMakeBorder(l2, 2, 0, 0, 2, cv.BORDER_CONSTANT, value=(255, 0, 0))\n",
        "        l1_border = cv.copyMakeBorder(l1, 0, 2, 0, 2, cv.BORDER_CONSTANT, value=(255, 0, 0))\n",
        "        r2_border = cv.copyMakeBorder(r2, 2, 0, 2, 0, cv.BORDER_CONSTANT, value=(255, 0, 0))\n",
        "        r1_border = cv.copyMakeBorder(r1, 0, 2, 2, 0, cv.BORDER_CONSTANT, value=(255, 0, 0))\n",
        "\n",
        "        cv.putText(l2_border, \"Brightness: \" + str(np.round(brightness_l2)), (0, 25), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv.LINE_AA,)\n",
        "        cv.putText(l1_border, \"Brightness: \" + str(np.round(brightness_l1)), (0, 25), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv.LINE_AA,)\n",
        "        cv.putText(r2_border, \"Brightness: \" + str(np.round(brightness_r2)), (0, 25), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv.LINE_AA,)\n",
        "        cv.putText(r1_border, \"Brightness: \" + str(np.round(brightness_r1)), (0, 25), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv.LINE_AA,)\n",
        "\n",
        "        numpy_vertical_l_brightness = np.vstack((l1_border, l2_border))\n",
        "        numpy_vertical_r_brightness = np.vstack((r1_border, r2_border))\n",
        "        grayscale_frame_brightness = np.hstack((numpy_vertical_l_brightness, numpy_vertical_r_brightness))\n",
        "        cv.imshow(\"Current Brightness\", grayscale_frame_brightness)\n",
        "\n",
        "    return grayscale_frame    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Debug-Modus - Slider\n",
        "\n",
        "Diese Slider stellen eine weitere Zusatzfunktion unserer Implementierung dar. Wenn der Debug-Modus aktiviert ist, können mit Hilfe dieser Slider Einstellungen, wie z.B. die Werte der Farbmasken für jede Farbe, verändert werden.\n",
        "\n",
        "Damit ließen sich die Werte der jeweiligen Masken an einem Live-Bild leichter optimieren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def on_change(value):\n",
        "    \"\"\"\n",
        "    Callback function that gets called whenever one of the slider inside the Options window is changed.\n",
        "    Global variable next_frame is set to true and therefore onscreen images get updated.\n",
        "    \"\"\"\n",
        "    global next_frame\n",
        "    next_frame = True\n",
        "    \n",
        "def init_sliders():\n",
        "    \"\"\"Creates option window with sliders for mask range and frame selection.\"\"\"\n",
        "    cv.namedWindow(\"Options\", cv.WINDOW_NORMAL)\n",
        "    \n",
        "    global frames\n",
        "    global white_lower\n",
        "    global white_upper\n",
        "    global yellow_lower\n",
        "    global yellow_upper\n",
        "\n",
        "    cv.createTrackbar('Autoplay', 'Options', 0, 1, on_change)\n",
        "    \n",
        "    cv.createTrackbar('Frame', 'Options', 0, len(frames), on_change)\n",
        "\n",
        "    cv.createTrackbar('w_low1', 'Options', 0, 255, on_change)\n",
        "    cv.createTrackbar('w_low2', 'Options', 0, 255, on_change)\n",
        "    cv.createTrackbar('w_low3', 'Options', 0, 255, on_change)\n",
        "\n",
        "    cv.createTrackbar('w_up1', 'Options', 0, 255, on_change)\n",
        "    cv.createTrackbar('w_up2', 'Options', 0, 255, on_change)\n",
        "    cv.createTrackbar('w_up3', 'Options', 0, 255, on_change)\n",
        "\n",
        "    cv.createTrackbar('y_low1', 'Options', 0, 255, on_change)\n",
        "    cv.createTrackbar('y_low2', 'Options', 0, 255, on_change)\n",
        "    cv.createTrackbar('y_low3', 'Options', 0, 255, on_change)\n",
        "\n",
        "    cv.createTrackbar('y_up1', 'Options', 0, 255, on_change)\n",
        "    cv.createTrackbar('y_up2', 'Options', 0, 255, on_change)\n",
        "    cv.createTrackbar('y_up3', 'Options', 0, 255, on_change)\n",
        "\n",
        "\n",
        "    cv.setTrackbarPos('w_low1', 'Options', white_lower[0])\n",
        "    cv.setTrackbarPos('w_low2', 'Options', white_lower[1])\n",
        "    cv.setTrackbarPos('w_low3', 'Options', white_lower[2])\n",
        "\n",
        "    cv.setTrackbarPos('w_up1', 'Options', white_upper[0]) \n",
        "    cv.setTrackbarPos('w_up2', 'Options', white_upper[1])\n",
        "    cv.setTrackbarPos('w_up3', 'Options', white_upper[2])\n",
        "\n",
        "    cv.setTrackbarPos('y_low1', 'Options', yellow_lower[0])\n",
        "    cv.setTrackbarPos('y_low2', 'Options', yellow_lower[1])\n",
        "    cv.setTrackbarPos('y_low3', 'Options', yellow_lower[2])\n",
        "\n",
        "    cv.setTrackbarPos('y_up1', 'Options', yellow_upper[0])\n",
        "    cv.setTrackbarPos('y_up2', 'Options', yellow_upper[1])\n",
        "    cv.setTrackbarPos('y_up3', 'Options', yellow_upper[2])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Main\n",
        "\n",
        "Folgender Abschnitt zeigt den gesamten Ablauf der Fahrspurerkennung.\n",
        "\n",
        "In Zeile 2 lässt sich festlegen, in welchem der gegebenen Videos die Spurerkennung durchgeführt werden soll. Zur Auswahl stehen: \"project\", \"challenge\" und \"harder_challenge\".\n",
        "\n",
        "Nach einem Durchlauf des Programms findet sich am Ende des Notebooks die oben angesprochene Tabelle, die den zeitlichen Anteil der einzelnen Funktionen zeigt.\n",
        "\n",
        "Zusätzlich zu den oben beschriebenen Slidern wurde die im Debug-Modus aktive Funktion implementiert, mit der Taste \"w\" einen Frame weiter und mit \"s\" einen Frame zurück zu springen. Auch hier lieferte diese Funktion den elementaren Vorteil, Fehler bei der Spurerkennung besser zu lokalisieren und Optimierungen zu finden.\n",
        "\n",
        "<p style=\"color: red\"> ------------------------------------------------------------ Evtl. verschieben ------------------------------------------------------------ </p>\n",
        "\n",
        "Um die Erkennung der Fahrspur noch weiter zu verbessern, wurde das Frame in vier Quadranten eingeteilt. Diese Quadranten werden als jeweils einzelne Frames betrachtet. Somit kann die Helligkeitsanpassung schneller auf Helligkeitsschwankungen ragieren. Damit können, beispielsweise im \"challenge\"-Video unter der Brücke, vorherige Fahrbahnmarkierungen länger und folgenden Fahrbahnmarkierungen früher erkannt werden.\n",
        "\n",
        "<p style=\"color: red\"> ------------------------------------------------------------ Evtl. verschieben ------------------------------------------------------------ </p>\n",
        "Um zu demonstrieren, dass unsere Optimierung mit Hilfe der Helligkeitsanpassung und dem Aufteilen des Frames in vier Quadranten eine wirkliche Optimierung darstellt, wurden beide Videos (\"project_video\" und \"challenge_video\") einmal mit und einmal ohne Helligkeitsanpassung durchlaufen und abgespeichert. Im aktuellen Code ist die Helligkeitserkennung jedoch standardmäßig aktiviert.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Open video file\n",
        "capture = cv.VideoCapture(\"./img/Udacity/\" + video_file + \"_video.mp4\")\n",
        "\n",
        "# Check if camera opened successfully\n",
        "if capture.isOpened() == False:\n",
        "    print(\"Error opening video stream or file\")\n",
        "\n",
        "# variables for frame selection\n",
        "next_frame = True\n",
        "position = 0\n",
        "\n",
        "# Read all frames. This is necessary to be able to use the slider to select a frame\n",
        "frames = []\n",
        "while capture.isOpened():\n",
        "    ret, image = capture.read()\n",
        "\n",
        "    # Check if there is another frame\n",
        "    if image is None:\n",
        "        break\n",
        "\n",
        "    frames.append(image)\n",
        "# Release video capture\n",
        "capture.release()  \n",
        "\n",
        "#play like a video in debug mode\n",
        "autoplay = False\n",
        "\n",
        "# define mask ranges\n",
        "yellow_lower = np.array([150, 100, 140])\n",
        "yellow_upper = np.array([255, 140, 200])\n",
        "\n",
        "white_lower = np.array([0, 180, 0])\n",
        "white_upper = np.array([254, 254, 254])\n",
        "\n",
        "if DEBUG_MODE: # enable sliders to change mask ranges in real time \n",
        "    init_sliders()\n",
        "    autplay = False\n",
        "\n",
        "# cache between frames\n",
        "old_left_pts = None\n",
        "old_right_pts = None\n",
        "\n",
        "# reset time analysis\n",
        "time_measurements = {}\n",
        "\n",
        "# Start timer for fps counter\n",
        "start_timer = time.time() - 0.01\n",
        "frame_count = -1\n",
        "\n",
        "# Read every frame\n",
        "while position < len(frames):\n",
        "    image = frames[position]\n",
        "\n",
        "    # Check if there is another frame\n",
        "    if image is None:\n",
        "        break\n",
        "\n",
        "    if next_frame:\n",
        "\n",
        "        if DEBUG_MODE: # update sliders\n",
        "            white_lower[0] = cv.getTrackbarPos('w_low1', 'Options')\n",
        "            white_lower[1] = cv.getTrackbarPos('w_low2', 'Options')\n",
        "            white_lower[2] = cv.getTrackbarPos('w_low3', 'Options')\n",
        "\n",
        "            white_upper[0] = cv.getTrackbarPos('w_up1', 'Options')\n",
        "            white_upper[1] = cv.getTrackbarPos('w_up2', 'Options')\n",
        "            white_upper[2] = cv.getTrackbarPos('w_up3', 'Options')\n",
        "\n",
        "            yellow_lower[0] = cv.getTrackbarPos('y_low1', 'Options')\n",
        "            yellow_lower[1] = cv.getTrackbarPos('y_low2', 'Options')\n",
        "            yellow_lower[2] = cv.getTrackbarPos('y_low3', 'Options')\n",
        "\n",
        "            yellow_upper[0] = cv.getTrackbarPos('y_up1', 'Options')\n",
        "            yellow_upper[1] = cv.getTrackbarPos('y_up2', 'Options')\n",
        "            yellow_upper[2] = cv.getTrackbarPos('y_up3', 'Options')\n",
        "\n",
        "        # Calculate Frame rate\n",
        "        frame_count += 1\n",
        "        elapsed_time = time.time() - start_timer\n",
        "        frame_rate = frame_count / elapsed_time\n",
        "        \n",
        "        start_time_measurement(\"frame\")\n",
        "        \n",
        "        # --------- Preprocessing ---------\n",
        "        start_time_measurement(\"Preprocessing\")\n",
        "        image = undistort_image_remap(image)\n",
        "        frame_undistorted = image.copy()\n",
        "        image = warp_image_udacity(image)\n",
        "        end_time_measurement(\"Preprocessing\")\n",
        "\n",
        "        # ---------  Brightness correction ---------\n",
        "        image = correct_Brightness_and_Split(image)\n",
        "\n",
        "        # ---------  Masking ---------\n",
        "        start_time_measurement(\"masking\")\n",
        "        frame_mask = lane_detection(image, white_lower, white_upper, yellow_lower, yellow_upper)\n",
        "        end_time_measurement(\"masking\")\n",
        "\n",
        "        # --------- Sliding Windows ---------\n",
        "        start_time_measurement(\"sliding windows\")\n",
        "        lefts, rights = sliding_windows(frame_mask, minimum_whites=margin)\n",
        "        end_time_measurement(\"sliding windows\")\n",
        "\n",
        "        # --------- Calculate polynomial ---------\n",
        "        start_time_measurement(\"polynomial calculation\")\n",
        "        left_pts, right_pts = calculate_polynomial_points(lefts, rights)\n",
        "        end_time_measurement(\"polynomial calculation\")\n",
        "\n",
        "        # --------- draw plane from all sliding windows ---------\n",
        "        start_time_measurement(\"draw plane\")\n",
        "        if left_pts is not None:\n",
        "            old_left_pts = left_pts        \n",
        "        if right_pts is not None:\n",
        "            old_right_pts = right_pts\n",
        "        drawRecOnFrame(frame_undistorted, old_left_pts[0], old_right_pts[0])\n",
        "        if old_left_pts is  None and old_right_pts is  None:\n",
        "            print(\"No plane drawn\")\n",
        "        end_time_measurement(\"draw plane\")\n",
        "\n",
        "        # --------- Add frame rate to video ---------\n",
        "        if not DEBUG_MODE:\n",
        "            cv.putText(frame_undistorted, \"FPS: \" + str(round(frame_rate)), (0, 25), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv.LINE_AA,)\n",
        "            cv.putText(frame_undistorted, \"Frame: \" + str(position), (0, 50), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv.LINE_AA,)\n",
        "        else:\n",
        "            cv.putText(frame_undistorted, \"Frame: \" + str(position), (0, 25), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv.LINE_AA,)\n",
        "\n",
        "        cv.imshow(\"Frame\", frame_undistorted)\n",
        "\n",
        "        end_time_measurement(\"frame\")    \n",
        "        \n",
        "    pressedKey = cv.waitKey(1) & 0xFF\n",
        "    if pressedKey == ord('q'): # press 'q' to exit the video\n",
        "        break\n",
        "    \n",
        "    if autoplay or not DEBUG_MODE:\n",
        "        position += 1\n",
        "        next_frame = True\n",
        "    \n",
        "    if DEBUG_MODE: # DEBUG_MODE enables the user to navigate through the video with the keyboard ('w', 's') or the slider in the window\n",
        "        if cv.getTrackbarPos('Autoplay', 'Options') != int(autoplay):\n",
        "            autoplay = bool(cv.getTrackbarPos('Autoplay', 'Options'))\n",
        "\n",
        "        if autoplay:\n",
        "            cv.setTrackbarPos('Frame', 'Options', position)\n",
        "\n",
        "        if cv.getTrackbarPos('Frame', 'Options') != position:\n",
        "            position = cv.getTrackbarPos('Frame', 'Options')\n",
        "\n",
        "        if pressedKey == ord('w'):\n",
        "            position += 1\n",
        "            next_frame = True\n",
        "            cv.setTrackbarPos('Frame', 'Options', position)\n",
        "        elif pressedKey == ord('s') and position > 0:\n",
        "            position -= 1\n",
        "            next_frame = True\n",
        "            cv.setTrackbarPos('Frame', 'Options', position)\n",
        "        elif pressedKey == 32:\n",
        "            autoplay = not autoplay\n",
        "            cv.setTrackbarPos('Autoplay', 'Options', int(autoplay))\n",
        "\n",
        "# When everything done, close all windows\n",
        "cv.destroyAllWindows()\n",
        "\n",
        "# create time measurement analysis and print out results\n",
        "analyse_time_measurements()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fazit\n",
        "\n",
        "### <p style=\"color: red\"> Todo zusammen </p>\n",
        "\n",
        "- Perspektivtransformation erleichtert Spurerkennung enorm\n",
        "- Man kann sich leicht in Verbesserungen verrennen\n",
        "- Harder_challenge braucht mehr Verbesserung/Anpassung als gedacht\n",
        "- Sliding Windows geben beste Genauigkeit, Edge-Filter erkennen auch Asphaltwechsel -> erkennen zu viel, egal wie viel man mit Farb- und Morphologischen Filtern arbeitet\n",
        "- Reine Infos aus der Vorlesung reichen definitv nicht aus, um die Spurerkennung effektiv umzusetzen\n",
        "- Eigene Funktionen wie eine Zeitmessung sind essenziel für Optimierung an der richtigen Stelle\n",
        "\n",
        "\n",
        "## Ausblick\n",
        "- Multiprocessing der Helligkeitskorrektur und masking für einzele Quadranten"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.11.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
