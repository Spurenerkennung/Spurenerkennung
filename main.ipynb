{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Projekt zur Spurerkennung im Wahlfach \"Digitale Bildverarbeitung\" \n",
        "\n",
        "In diesem Jupyter-Notebook wird eine Fahrbahnmarkierungserkennung implementiert. Anhand von Bildern und Videos von Udacity und KITTI wird diese Erkennung auf verschiedenste Weise getestet.\n",
        "\n",
        "Um eine Fahrbahn erkennen zu können müssen folgende Schritte während des Programmablaufs abgearbeitet werden:\n",
        "\n",
        "    - Kamerakalibrierung\n",
        "    - Perspektivtransformation\n",
        "    - Maskieren des Bildes mit gelben und weißen Farbmasken\n",
        "    - \"Sliding Windows\"\n",
        "    - Berechnung der Polynome für die Eingrenzung der Fahrbahn\n",
        "    - Rücktransformation der berechneten Punkte der Polynome\n",
        "    - Einzeichnen der Fläche zwischen den Polynomen\n",
        "\n",
        "Im folgenden werden die jeweils verwendeten Funktionen genauer erklärt, am Ende finden sich die Bilder zur Kamerakalibrierung sowie zur Spurerkennung.\n",
        "\n",
        "<sup>Bearbeitet wurde das Projekt von Alexander Schulte(), Edmund Krain () und Marcel Fleck (9611872)</sup>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import time\n",
        "import glob\n",
        "from prettytable import PrettyTable\n",
        "from deprecated import deprecated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Important flags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# video_file = \"project\"\n",
        "video_file = \"challenge\"\n",
        "\n",
        "DEBUG_MODE = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Time Measurement\n",
        "\n",
        "Diese Funktionen dienen der Bestimmung von zeitaufwändigen Arbeitsschritten, um eine Verbesserung der Framerate zu erleichtern.\n",
        "\n",
        "In einem Array wird ein Objekt hinterlegt, in dem der Name, die Start- und Endzeit sowie die Anzahl an Aufrufen hinterlegt wird (siehe ```start_time_measurement``` und ```end_time_measurement```).\n",
        "\n",
        "Diese Informationen werden dann in einer Tabelle nach Beendigung des Programmdurchlaufs visualisiert (siehe ```analyse_time_measurement```).\n",
        "\n",
        "Abbildung 1 zeigt ein Beispile für eine solche Tabelle:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"img/documentation/timemeasurement.png\" height=\"35%\" width=\"35%\" />\n",
        "  <br>\n",
        "  <em>Abbildung 1: Tabelle zur Darstellung der Zeitmessungsergebnisse</em>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_measurements = {}\n",
        "\n",
        "def start_time_measurement(eventName):\n",
        "    \"\"\"\n",
        "    Use this function to start a time measurement for a specific event.\n",
        "    A end_time_measurement() call with the same name must be called before the next start_time_measurement() call.\n",
        "    \"\"\"\n",
        "    add = False\n",
        "    if eventName not in time_measurements:\n",
        "        time_measurements[eventName] = {\n",
        "            \"name\": eventName,\n",
        "            \"start\": [],\n",
        "            \"end\": [],\n",
        "            \"count\": 0\n",
        "        }\n",
        "        add = True\n",
        "    elif len(time_measurements[eventName][\"start\"]) > len(time_measurements[eventName][\"end\"]):\n",
        "        print(f\"Time measure error: Event '{eventName}' not finished before reassignment!\")\n",
        "    else:\n",
        "       add = True\n",
        "    \n",
        "    #start measurement as late as possible\n",
        "    if add:\n",
        "        time_measurements[eventName][\"start\"].append(time.perf_counter_ns())\n",
        "\n",
        "def end_time_measurement(eventName):\n",
        "    \"\"\"\n",
        "    Use this function to end a time measurement for a specific event.\n",
        "    A time measurement with the same name must be started with start_time_measurement() before it can be ended.\n",
        "    \"\"\"\n",
        "    #end measurement as fast as possible\n",
        "    temp_time = time.perf_counter_ns()\n",
        "    if eventName not in time_measurements:\n",
        "        print(f\"Time measure error: Event '{eventName}' not defined!\")\n",
        "    elif len(time_measurements[eventName][\"end\"]) >= len(time_measurements[eventName][\"start\"]):\n",
        "        print(f\"Time measure error: Event '{eventName}' not started before reassignment!\")\n",
        "    else:\n",
        "        time_measurements[eventName][\"end\"].append(temp_time)\n",
        "        time_measurements[eventName][\"count\"] += 1\n",
        "    \n",
        "def analyse_time_measurements():\n",
        "    \"\"\"\n",
        "    Analyse time measurements and print them in a table.\n",
        "    \"\"\"\n",
        "    time_measurements_table = PrettyTable([\"Name\", \"Avg. [ms]\", \"Min. [ms]\", \"Max. [ms]\", \"Occurrences [compl.]\"])\n",
        "    time_measurements_table.align[\"Name\"] = \"l\"\n",
        "    time_measurements_table.align[\"Avg. [ms]\"] = \"r\"\n",
        "    time_measurements_table.align[\"Min. [ms]\"] = \"r\"\n",
        "    time_measurements_table.align[\"Max. [ms]\"] = \"r\"\n",
        "    time_measurements_table.align[\"Occurrences [compl.]\"] = \"r\"\n",
        "    for key, event in time_measurements.items():\n",
        "        timings = []\n",
        "        if len(event[\"start\"]) != len(event[\"end\"]):\n",
        "            print(f\"Time measure error: Event '{key}' has different amounts of values for start and end times!\")\n",
        "            continue\n",
        "        \n",
        "        for i in range(len(event[\"start\"])):\n",
        "            timing = (event[\"end\"][i] - event[\"start\"][i])\n",
        "            #exclude 0 values\n",
        "            if timing >= 0:\n",
        "                timing = timing / (1000 * 1000) #convert from ns to ms\n",
        "                timings.append(timing)\n",
        "\n",
        "        event[\"min\"] = min(timings) \n",
        "        event[\"max\"] = max(timings)\n",
        "        event[\"avg\"] = sum(timings) / len(event[\"start\"])\n",
        "\n",
        "        time_measurements_table.add_row([key, '{0:.2f}'.format(event[\"avg\"]), '{0:.2f}'.format(event[\"min\"]), '{0:.2f}'.format(event[\"max\"]), event[\"count\"]])\n",
        "        \n",
        "    print(time_measurements_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kamerakalibrierung\n",
        "\n",
        "Die Abbildung eines 3D-Objekts auf eine 2D-Bildebene mit einer Kamera wird durch die internen Größen einer Kamera beeinflusst. Dazu gehören zum Beispiel die Bildmitte, die Brennweite und auch Kameraverzerrungsparameter.\n",
        "\n",
        "Mit Hilfe der Kamerakalibrierung werden die sogenannten \"intrinsische\" und \"extrinsische\" Parameter berechnet. Intrinsische Parameter beschreiben die Kalibriermatrix (Kamera zu Pixel), während die extrinsischen Parameter die Beziehung zwischen dem Koordinatensystem der Kamera und dem Welt-Koordinatensystem beschreiben (Welt zu Kamera).\n",
        "\n",
        "Zusammen bilden diese Parameter eine Projektionsmatrix, die es ermöglicht, gemachte Bilder im Welt-Koordinatensystem darzustellen (die Bilder werden entzerrt). \n",
        "\n",
        "In folgendem Abschnitt wird die Kamera mit Hilfe von Schachbrett-Bildern kalibriert. Diese Aufgabe mit den Ergebnissen auf den Schachbrettbildern befinden sich im JupyterNotebook [Projekt_Spurerkennung_v2.jpynb](Projekt_Spurerkennung_v2.ipynb).\n",
        "\n",
        "\n",
        "Mit den unten definierten Funktionen (siehe ```undistort_image``` und ```undistort_image_remap```) werden dann die Frames der Videos anhand der berechneten Transformations-Matrix entzerrt, um eine tatsächliche Darstellung im Welt-Koordinatensystem zu erhalten. Eine Implementierung von zwei Funktionen lässt mit dem Erreichen von einer ungefähr 50% schnelleren Verarbeitung begründen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# size of chessboard, minimum error with (7, 6), but there were severe artifacts at the borders (see error calculation at the end)\n",
        "chessboard_x, chessboard_y = 9, 6\n",
        "\n",
        "# termination criteria\n",
        "criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
        "obj_points = np.zeros((chessboard_y * chessboard_x, 3), np.float32)\n",
        "obj_points[:, :2] = np.mgrid[0:chessboard_x, 0:chessboard_y].T.reshape(-1, 2)\n",
        "# Arrays to store object points and image points from all the images.\n",
        "object_points = []  # 3d point in real world space\n",
        "image_points = []  # 2d points in image plane.\n",
        "\n",
        "# use all calibration images\n",
        "images = glob.glob(\"./img/Udacity/calib/*.jpg\")\n",
        "for i, frame_name in enumerate(images):\n",
        "    image = cv.imread(frame_name)\n",
        "    image_gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Find the chess board corners\n",
        "    ret, corners = cv.findChessboardCorners(image_gray, (chessboard_x, chessboard_y), None)\n",
        "    # If found, add object points, image points (after refining them)\n",
        "    if ret == True:\n",
        "        object_points.append(obj_points)\n",
        "        corners2 = cv.cornerSubPix(\n",
        "            image_gray, corners, (11, 11), (-1, -1), criteria\n",
        "        )  # improve accuracy of corners\n",
        "        image_points.append(corners)\n",
        "\n",
        "ret, camera_matrix, dist_coefficient, rvecs, tvecs = cv.calibrateCamera(\n",
        "    object_points, image_points, image_gray.shape[::-1], None, None\n",
        ")\n",
        "\n",
        "image_height, image_width = image.shape[:2]\n",
        "new_camera_matrix, roi = cv.getOptimalNewCameraMatrix(camera_matrix, dist_coefficient, (image_width, image_height), 1, (image_width, image_height))\n",
        "\n",
        "\n",
        "# ------- define functions for image processing -------\n",
        "def undistort_image(img):\n",
        "    img_undistorted = cv.undistort(img, camera_matrix, dist_coefficient, None, new_camera_matrix)\n",
        "    # crop the image\n",
        "    x, y, w, h = roi\n",
        "    return img_undistorted[y : y + h, x : x + w]\n",
        "\n",
        "# ~50% faster than undistort_image()\n",
        "def undistort_image_remap(img):\n",
        "    h, w = img.shape[:2]\n",
        "    \n",
        "    map_x, map_y = cv.initUndistortRectifyMap(camera_matrix, dist_coefficient, None, new_camera_matrix, (w, h), 5)\n",
        "    dst = cv.remap(img, map_x, map_y, cv.INTER_LINEAR)\n",
        "    # crop the image\n",
        "    x, y, image_width, image_height = roi\n",
        "    return dst[y : y + image_height, x : x + image_width]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Perspektivtransformation\n",
        "\n",
        "Die Perspektivtransformation wird verwendet, um die \"Region of Interest\" festzulegen. Dadurch muss nicht mehr das gesamte Bild verarbeitet werden, sondern es kann nur ein der Bereich, in dem sich die Fahrspur befindet, zur Verarbeitung genutzt werden.\n",
        "\n",
        "Hierbei werden vier feste Punkte im originalen Bild ausgewählt. Diese Punkte werden dann in einem Bild mit den selben Dimensionen wie das Originalbild dargestellt, um eine rechteckige Beziehung zwischen den Punkten herzustellen (siehe Abbildung 1). Die Transformation geschieht mit der Funktion ```warp_image_udacity```.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"img/documentation/perspectiveTransformation.png\" />\n",
        "  <br>\n",
        "  <em>Abbildung 2: Beispiel für implementierte Perspektivtransformation</em>\n",
        "</p>\n",
        "\n",
        "Um die mit einer später erklärten Funktion (```sliding_windows```) in dem transformierten Bild gefundenen Punkte in dem originalen Bild anzeigen zu können, müssen diese Punkte zurücktransformiert werden. Um Rechenleistung zu sparen, werden tatsächlich nur die Punkte transformiert (siehe ```rewarp_points_udacity```), bei einem ersten Ansatz wurde das gesamte Bild zurücktransformiert (siehe ```rewarp_image_udacity```). \n",
        "\n",
        "- Bild nach warp verkleinern, da ein großteil des Bildes aus wenigen Pixeln entsteht -> nicht gemacht weil probleme beim rückwarpen\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# udacity images\n",
        "if video_file == \"harder_challenge\":\n",
        "    src_udacity = np.float32([[20, 628], [191+100, 404], [1200, 628], [1021-100, 404]])\n",
        "    dst_udacity = np.float32([[150, 720], [150, 10], [1000, 720], [1000, 10]])\n",
        "else:\n",
        "    src_udacity = np.float32([[191, 628], [531, 404], [1021, 628], [681, 404]])\n",
        "    dst_udacity = np.float32([[150, 720], [150, 10], [1000, 720], [1000, 10]])\n",
        "\n",
        "M_warp = cv.getPerspectiveTransform(src_udacity, dst_udacity)\n",
        "M_rewarp = cv.getPerspectiveTransform(dst_udacity, src_udacity)\n",
        "\n",
        "\n",
        "def warp_image_udacity(img):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        img (_type_): _description_\n",
        "\n",
        "    Returns:\n",
        "        _type_: _description_\n",
        "    \"\"\"\n",
        "    image = cv.warpPerspective(img, M_warp, (img.shape[1], img.shape[0]))\n",
        "    if DEBUG_MODE:\n",
        "        #Draw a red circle with zero radius and -1 for filled circle\n",
        "        for src in src_udacity:\n",
        "            image2 = cv.circle(img, np.int32(src), radius=0, color=(0, 0, 255), thickness=5)\n",
        "        cv.imshow(\"Transformed\", image2)\n",
        "    return image\n",
        "\n",
        "@deprecated\n",
        "def rewarp_image_udacity(img):\n",
        "    img = cv.warpPerspective(img, M_rewarp, (img.shape[1], img.shape[0]), cv.WARP_INVERSE_MAP)\n",
        "    return img\n",
        "\n",
        "def rewarp_points_udacity(points):\n",
        "    \"\"\"\n",
        "    Rewarp points from warped image coordinates to original image coordinates.\n",
        "    \n",
        "    Args:\n",
        "        points (np.array): float32 array \n",
        "        \n",
        "    @return: points in original image coordinates\n",
        "    \"\"\"\n",
        "    return cv.perspectiveTransform(points, M_rewarp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sliding Windows\n",
        "\n",
        "Die Erkennung der Fahrbahnmarkierungen wird mit den sogenannten \"Sliding Windows\" umgesetzt. \n",
        "\n",
        "Kurz gefasst sind \"Sliding Windows\" kleine Kästchen, die anhand der Helligkeit von Pixel ihren Mittelpunkt anpassen. Damit kann beispielsweise eine weiße Linie auf schwarzem Hintergrund fast perfekt verfolgt werden. \n",
        "\n",
        "Detailreicher beginnt die ```sliding_windows``` Funktion damit, anhand des Histogramms eines Frames die initialen Mittelpunkte der ersten Windows festzulegen. Anhand dieser Mittelpunkte werden eine angegebene Anzahl an Windows erzeugt, die aufeinander gestapelt werden. Das bedeutet, der X-Wert der Windows bleibt gleich, der Y-Wert passt sich an.\n",
        "\n",
        "Für jedes Window wird nun geprüft, wie viele hellen Pixel von dem jeweiligen Window eingegrenzt werden. Wenn der Wert der gefundenen Pixel das angegebene Minimum unterschreitet, wird der Mittelpunkt des jeweiligen Windows angepasst.\n",
        "\n",
        "Ein Beispiel für diese \"Sliding Windows\" zeigt Abbildung 3.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"img/documentation/slidingWindows.png\" width=\"40%\" height=\"40%\"/>\n",
        "  <br>\n",
        "  <em>Abbildung 3: Beispiel für die Darstellung der \"Sliding Windows\"</em>\n",
        "</p>\n",
        "\n",
        "Damit kann die Fahrspur bei einer Veränderung leicht weiterverfolgt und markiert werden. Die hier gefundenen Mittelpunkte bilden die Grundlage für die Polynomberechnung."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "number_windows = 50\n",
        "margin = 50\n",
        "def sliding_windows(frame, window_width=200, minimum_whites=30):\n",
        "    \"\"\"_summary_ todo\n",
        "\n",
        "    Args:\n",
        "        frame (image): input frame with masked lane lines\n",
        "        window_width (int, optional): width of windows. Defaults to 200.\n",
        "        minimum_whites (int, optional): todo. Defaults to 30.\n",
        "\n",
        "    Returns:\n",
        "        lefts, rights (np.array): returns left and right points as 2d array of sliding window centers\n",
        "    \"\"\"\n",
        "    # Histogram for image\n",
        "    hist = np.sum(frame[frame.shape[0]//2:, :], axis=0)\n",
        "        \n",
        "    # Take peaks from left and right side of histogram for starting points and add half margin\n",
        "    mid_point_x = np.int32(hist.shape[0] // 2)\n",
        "    left_x_start = np.argmax(hist[:mid_point_x]) - window_width // 2\n",
        "    right_x_start = np.argmax(hist[mid_point_x:]) + mid_point_x + window_width // 2\n",
        "    # Window height based on number of windows\n",
        "    window_height = np.int32(frame.shape[0] // number_windows)\n",
        "    \n",
        "    # Calc points that are not zero in images\n",
        "    nonzero = frame.nonzero()\n",
        "    nonzero_y = np.array(nonzero[0])\n",
        "    nonzero_x = np.array(nonzero[1])\n",
        "    \n",
        "    # Initialize current positions for windows\n",
        "    left_x_current = left_x_start\n",
        "    right_x_current = right_x_start\n",
        "\n",
        "    # Initialize values to be returned -> centers of windows\n",
        "    lefts_good = np.empty((0,2), dtype=np.int32)\n",
        "    rights_good = np.empty((0,2), dtype=np.int32)\n",
        "\n",
        "    # Go through every window\n",
        "    for window in range(number_windows):\n",
        "        # Identify window boundaries in x and y (and right and left)\n",
        "        win_y_low = frame.shape[0] - (window + 1) * window_height\n",
        "        win_y_high = frame.shape[0] - window*window_height\n",
        "        y_mid = (win_y_low + win_y_high) // 2\n",
        "        \n",
        "        # Calculate boundaries of the window\n",
        "        win_xleft_low = left_x_current - window_width  \n",
        "        win_xleft_high = left_x_current + window_width  \n",
        "        win_xright_low =  right_x_current - window_width \n",
        "        win_xright_high = right_x_current + window_width  \n",
        "        \n",
        "        # Identify the pixels that are not zero within window\n",
        "        left_inds = ((nonzero_y >= win_y_low ) & (nonzero_y < win_y_high) & (nonzero_x >= win_xleft_low) & (nonzero_x < win_xleft_high)).nonzero()[0]\n",
        "        right_inds = ((nonzero_y >= win_y_low ) & (nonzero_y < win_y_high) & (nonzero_x >= win_xright_low) & (nonzero_x < win_xright_high)).nonzero()[0]\n",
        "        \n",
        "        # If more than minimum pixels are found -> recenter next window\n",
        "        if len(left_inds) > minimum_whites:\n",
        "            left_x_current = np.int32(np.mean(nonzero_x[left_inds]))\n",
        "            lefts_good = np.concatenate((lefts_good, [[left_x_current, y_mid]]))\n",
        "            if DEBUG_MODE:\n",
        "                cv.rectangle(frame, (left_x_current - margin, win_y_low),(left_x_current + margin, win_y_high),(255, 255, 255), 2)\n",
        "        if len(right_inds) > minimum_whites:\n",
        "            right_x_current = np.int32(np.mean(nonzero_x[right_inds]))\n",
        "            rights_good = np.concatenate((rights_good, [[right_x_current, y_mid]]))\n",
        "            if DEBUG_MODE:\n",
        "                cv.rectangle(frame, (right_x_current - margin, win_y_low),(right_x_current + margin, win_y_high),(255, 255, 255), 2)\n",
        "\n",
        "    return lefts_good, rights_good"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Polynome\n",
        "\n",
        "Um eine Veränderung der Fahrspur, z.B. bei Kurven, angemessen darstellen zu können, werden anhand der von ```sliding_windows``` gefundenen Punkte Polynome berechnet.\n",
        "\n",
        "Diese bilden dann die rechten und linken Grenzen der einzufärbenden Fläche. Dadurch lassen sich selbst kleine Änderungend er Fahrspur ordentlich darstellen. (siehe Abbildung 4)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"img/documentation/polynoms.png\" width=\"40%\" height=\"40%\"/>\n",
        "  <br>\n",
        "  <em>Abbildung 4: Darstellung der berechneten Polynome auf der Fahrspur</em>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  calculate polynomial from rewarped points\n",
        "max_polynom_curvature_value = 0.005 # defines how much the polynom can be curved before it is considered faulty\n",
        "\n",
        "def calculate_polynomial_points(lefts, rights):\n",
        "    \"\"\"\n",
        "    calculate polynomial from rewarped points and then return list of points on the polynomial\n",
        "    @return: array of points for left and right polynomial\n",
        "    \"\"\"\n",
        "    # flags for faulty polynom sides\n",
        "    is_left_faulty, is_right_faulty = False, False\n",
        "    \n",
        "    # check if there are enough points\n",
        "    if len(lefts) < 2 or len(rights) < 2:\n",
        "        return None, None\n",
        "    \n",
        "    # calculate polynomial from rewarped points\n",
        "    left_polynom_values = np.polyfit(lefts[:,1], lefts[:,0], 2)\n",
        "    right_polynom_values = np.polyfit(rights[:,1], rights[:,0], 2)\n",
        "\n",
        "    # 750 as x length of polynom\n",
        "    x_axis = np.linspace(0, 750, 750) \n",
        "    # check if polynom is valid (not too steeply curved)\n",
        "    if (left_polynom_values[0] > max_polynom_curvature_value or left_polynom_values[0] < -max_polynom_curvature_value):\n",
        "        is_left_faulty = True\n",
        "    if (right_polynom_values[0] > max_polynom_curvature_value or right_polynom_values[0] < -max_polynom_curvature_value):\n",
        "        is_right_faulty = True\n",
        "    if is_left_faulty and is_right_faulty:\n",
        "        return None, None\n",
        "    \n",
        "    # 750 as x length of polynom\n",
        "    x_axis = np.linspace(0, 750, 750)\n",
        "\n",
        "    # calculate y values for left and right line\n",
        "    left_line_y = left_polynom_values[0]*x_axis**2 + left_polynom_values[1]*x_axis + left_polynom_values[2]\n",
        "    right_line_y = right_polynom_values[0]*x_axis**2 + right_polynom_values[1]*x_axis + right_polynom_values[2]\n",
        "\n",
        "    # array of points for left and right line from x and y values\n",
        "    left_pts = np.array([np.transpose(np.vstack([left_line_y, x_axis]))])\n",
        "    right_pts = np.array([np.transpose(np.vstack([right_line_y, x_axis]))])\n",
        "    \n",
        "    # loop through all points and check if they are to close or to far away from each other\n",
        "    # (if they are to close -> most likely a interception point)\n",
        "    for i in range(len(left_pts[0])):\n",
        "        y_distance = abs(left_pts[0][i][0] - right_pts[0][i][0])\n",
        "        if y_distance < 10 or y_distance > 1000:\n",
        "            return None, None\n",
        "\n",
        "    # only return non faulty polynom points\n",
        "    return left_pts if not is_left_faulty else None, right_pts if not is_right_faulty else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Enzeichen der Fahrspur\n",
        "\n",
        "Mit Hilfe der Funktion ```drawRecOnFrame``` wird dann die Fläche zwischen den Polynomen eingezeichnet (siehe Abbildung 5).\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"img/documentation/filledLane.png\" width=\"40%\" height=\"40%\"/>\n",
        "  <br>\n",
        "  <em>Abbildung 5: Markierung der Fahrbahn mit Polynomen als Grenzen</em>\n",
        "</p>\n",
        "\n",
        "Um die Spurerkennung stabiler zu gestalten, werden bei einer zu geringen Anzahl an gefundenen Punkten keine neue Polynome berechnet. Die Spur wird dann anhand der zuletzt berechneten Polynome eingezeichnet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def drawRecOnFrame(frame, left_pts, right_pts):\n",
        "    \"\"\"\n",
        "    Rewarp points to original image coordinates and draw rectangle on frame.\n",
        "    \n",
        "    input: array of points from left and right lane marking in warped image\n",
        "    e.g.:\n",
        "      [[ 281   39]\n",
        "      [ 971  163]\n",
        "      [ 958  101]]\n",
        "    \"\"\"\n",
        "    if len(left_pts) + len(right_pts) > 3:\n",
        "      borderPoints = np.concatenate((np.flip(left_pts, axis=0), right_pts))\n",
        "      borderPointsRewarped = rewarp_points_udacity(np.array([borderPoints], dtype=np.float32))\n",
        "      borderPointsRewarpedInt = borderPointsRewarped.astype(int)\n",
        "      # draw final polygon in frame\n",
        "      cv.drawContours(frame, borderPointsRewarpedInt, -1, (0,255,0), -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Masking\n",
        "\n",
        "Mit ```applyMasks``` werden bestimmte Farbmasken, genauer eine für Weiß und eine für Gelb, auf das Bild gelegt. Damit werden nur diese zwei Farben in den Bildern hervorgehoben bzw. dargestellt. \n",
        "\n",
        "Diese Funktion ist inzwischen veraltet, da die Funktion ```lane_detection``` eine verbesserte Implementierung mit neuen Filtern zur Verfügung stellt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@deprecated\n",
        "def applyMasks(frame):\n",
        "    \"\"\"\n",
        "    Apply masks to frame and return frame with only lane marking.\n",
        "    \"\"\"\n",
        "    ## convert to hsv\n",
        "    hls_frame = cv.cvtColor(frame, cv.COLOR_BGR2HLS)\n",
        "\n",
        "    ## mask for white\n",
        "    white_mask = cv.inRange(hls_frame, (0, 200, 0), (255, 255,255))\n",
        "\n",
        "    ## mask for yellow\n",
        "    # yellow_mask = cv.inRange(hls_frame, (20,90,200), (26, 255, 255))\n",
        "    yellow_mask = cv.inRange(hls_frame, (10,0,100), (40, 255, 255))\n",
        "\n",
        "    ## final mask and masked\n",
        "    mask = cv.bitwise_or(white_mask, yellow_mask)\n",
        "    frame = cv.bitwise_and(frame,frame, mask=mask)\n",
        "\n",
        "    return frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lane_detection(frame, white_lower, white_upper, yellow_lower, yellow_upper):\n",
        "    \"\"\"_summary_\n",
        "\n",
        "    Args:\n",
        "        frame (_type_): _description_\n",
        "        white_lower (_type_): _description_\n",
        "        white_upper (_type_): _description_\n",
        "        yellow_lower (_type_): _description_\n",
        "        yellow_upper (_type_): _description_\n",
        "\n",
        "    Returns:\n",
        "        _type_: _description_\n",
        "    \"\"\"\n",
        "    frame = cv.GaussianBlur(frame, (5, 5), 0)\n",
        "    frame_hls = cv.cvtColor(frame, cv.COLOR_BGR2HLS)\n",
        "\n",
        "    white_mask = cv.inRange(frame_hls, white_lower, white_upper)\n",
        "    white_mask[:, 0:200] = 0\n",
        "\n",
        "    frame_lab = cv.cvtColor(frame, cv.COLOR_BGR2LAB)\n",
        "    yellow_mask = cv.inRange(frame_lab, yellow_lower, yellow_upper)\n",
        "    yellow_mask[:, 1000:] = 0\n",
        "    combined_mask = cv.bitwise_or(white_mask, yellow_mask)\n",
        "    \n",
        "    if DEBUG_MODE: # needs frame at this point\n",
        "        frame_luv = cv.cvtColor(frame, cv.COLOR_BGR2LUV)\n",
        "        greenImg = np.zeros(frame.shape, frame.dtype)\n",
        "        redImg = np.zeros(frame.shape, frame.dtype)\n",
        "\n",
        "    kernel = cv.getStructuringElement(cv.MORPH_RECT, (13, 13))\n",
        "    lanes_yellow = cv.morphologyEx(frame_lab[:, :, 2], cv.MORPH_TOPHAT, kernel)\n",
        "\n",
        "    ret, lanes_yellow = cv.threshold(lanes_yellow, thresh=2, maxval=255, type=cv.THRESH_BINARY)\n",
        "    lanes_yellow = cv.morphologyEx(lanes_yellow, cv.MORPH_OPEN, np.array([[0,1,0],[1,1,1],[0,1,0]], 'uint8'), iterations=4)\n",
        "\n",
        "    combined_mask = cv.bitwise_or(combined_mask, lanes_yellow)\n",
        "\n",
        "    combined_mask = cv.morphologyEx(combined_mask, cv.MORPH_OPEN, np.array([[0,1,0],[1,1,1],[0,1,0]], 'uint8'), iterations=2)\n",
        "    frame = cv.bitwise_and(frame, frame, mask=combined_mask)\n",
        "    frame = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "    \n",
        "    if DEBUG_MODE: # show multiple debug images\n",
        "        cv.imshow(\"LUV\", frame_luv[:,:,2])\n",
        "        \n",
        "        redImg[:,:] = (0, 0, 255)\n",
        "        redMask = cv.bitwise_and(redImg, redImg, mask=yellow_mask)\n",
        "\n",
        "        greenImg[:,:] = (0, 255, 0)\n",
        "        greenMask = cv.bitwise_and(greenImg, greenImg, mask=white_mask)\n",
        "        masks = redMask + greenMask\n",
        "        cv.imshow(\"Masks\", masks)\n",
        "        \n",
        "        lanes = cv.morphologyEx(frame_hls[:, :, 1], cv.MORPH_BLACKHAT, kernel)\n",
        "        ret, lanes = cv.threshold(lanes, thresh=10, maxval=255, type=cv.THRESH_BINARY)\n",
        "        cv.imshow(\"BLACK_HAT_LANES\", lanes)\n",
        "\n",
        "        ret, canny = cv.threshold(frame_lab[:,:,2], thresh=140, maxval=255, type=cv.THRESH_BINARY)\n",
        "        half = canny.shape[1]//2\n",
        "        new_canny = canny[100:, :half] \n",
        "        cv.imshow(\"LAB_Canny\", new_canny)\n",
        "    \n",
        "        cv.imshow(\"LAB\", frame_lab[:,:,2])\n",
        "        cv.imshow(\"LAB_combined\", cv.bitwise_and(canny, lanes_yellow))\n",
        "        cv.imshow(\"HLS\", frame_hls)\n",
        "        \n",
        "        cv.imshow(\"TOP_HAT_LANES_YELLOW\", lanes_yellow)\n",
        "        cv.imshow(\"Final\", combined_mask)\n",
        "    return frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Helligkeitskorrektur\n",
        "\n",
        "Damit die Fahrbahnmarkierungen auch in dunklen Elementen der Videos zu erkennen sind, wurde eine dynamische Helligkeitsanpassung implementiert.\n",
        "\n",
        "Das Bild wird in HSV eingelesen und in die jeweiligen Bestandteile aufgeteilt. Wenn der Helligkeitswert des Frames dann kleiner oder größer ist als die angegebenen Grenzen, dann wird die Helligkeit des Bilds so angepasst, dass sie sich dann zwischen den gesetzten Grenzen befindet (vgl. Abbildung 5).\n",
        "\n",
        "<p align=\"middle\" float=\"left\">\n",
        "  <img src=\"img/documentation/brightnessCorrectionNormal.png\" width=\"40%\" height=\"40%\"/>\n",
        "  <img src=\"img/documentation/brightnessCorrectionCorrected.png\" width=\"40%\" height=\"40%\"/>\n",
        "  <br>\n",
        "  <em>Abbildung 5: Helligkeitskorrektur - Links: Original, Rechts: Korrigiert</em>\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def correct_Brightness(image):\n",
        "    \"\"\"Calculate the brightness of the overall image and adjust it if it is too bright or too dark.\n",
        "\n",
        "    Args:\n",
        "        image: input image\n",
        "\n",
        "    Returns:\n",
        "        image, brightness: corrected image and new overall brightness\n",
        "    \"\"\"\n",
        "    image_hsv = cv.cvtColor(image, cv.COLOR_BGR2HSV)\n",
        "    image_brightness_input = image_hsv[...,2].mean()\n",
        "    brightness_upper_limit = 150\n",
        "    brightness_lower_limit = 85\n",
        "    if image_brightness_input > brightness_upper_limit:\n",
        "        h, s, v = cv.split(image_hsv)\n",
        "        \n",
        "        lim = round(0 + image_brightness_input - brightness_upper_limit)\n",
        "        v[v < lim] = 0\n",
        "        v[v >= lim] -= lim\n",
        "\n",
        "        final_hsv = cv.merge((h, s, v))\n",
        "        image = cv.cvtColor(final_hsv, cv.COLOR_HSV2BGR)\n",
        "    \n",
        "    elif image_brightness_input < brightness_lower_limit:\n",
        "        h, s, v = cv.split(image_hsv)\n",
        "        \n",
        "        lim = round(255 + brightness_lower_limit - image_brightness_input)\n",
        "        v[v > lim] = 255\n",
        "        v[v <= lim] += lim\n",
        "\n",
        "        final_hsv = cv.merge((h, s, v))\n",
        "        image = cv.cvtColor(final_hsv, cv.COLOR_HSV2BGR)\n",
        "\n",
        "    image_hsv = cv.cvtColor(image, cv.COLOR_BGR2HSV)\n",
        "    image_brightness_new = image_hsv[...,2].mean()\n",
        "\n",
        "    return image, image_brightness_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Debugging slider"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def on_change(value):\n",
        "    \"\"\"\n",
        "    todo docstring\n",
        "    \"\"\"\n",
        "    global next_frame\n",
        "    next_frame = True\n",
        "    \n",
        "def init_sliders():\n",
        "    \"\"\"Creates option window with sliders for mask range and frame selection.\"\"\"\n",
        "    cv.namedWindow(\"Options\", cv.WINDOW_NORMAL)\n",
        "    \n",
        "    global frames\n",
        "    global white_lower\n",
        "    global white_upper\n",
        "    global yellow_lower\n",
        "    global yellow_upper\n",
        "    \n",
        "    cv.createTrackbar('Frame', 'Options', 0, len(frames), on_change)\n",
        "\n",
        "    cv.createTrackbar('white_lower_1', 'Options', 0, 255, on_change)\n",
        "    cv.createTrackbar('white_lower_2', 'Options', 0, 255, on_change)\n",
        "    cv.createTrackbar('white_lower_3', 'Options', 0, 255, on_change)\n",
        "\n",
        "    cv.createTrackbar('white_upper_1', 'Options', 0, 255, on_change)\n",
        "    cv.createTrackbar('white_upper_2', 'Options', 0, 255, on_change)\n",
        "    cv.createTrackbar('white_upper_3', 'Options', 0, 255, on_change)\n",
        "\n",
        "    cv.createTrackbar('yellow_lower_1', 'Options', 0, 255, on_change)\n",
        "    cv.createTrackbar('yellow_lower_2', 'Options', 0, 255, on_change)\n",
        "    cv.createTrackbar('yellow_lower_3', 'Options', 0, 255, on_change)\n",
        "\n",
        "    cv.createTrackbar('yellow_upper_1', 'Options', 0, 255, on_change)\n",
        "    cv.createTrackbar('yellow_upper_2', 'Options', 0, 255, on_change)\n",
        "    cv.createTrackbar('yellow_upper_3', 'Options', 0, 255, on_change)\n",
        "\n",
        "\n",
        "    cv.setTrackbarPos('white_lower_1', 'Options', white_lower[0])\n",
        "    cv.setTrackbarPos('white_lower_2', 'Options', white_lower[1])\n",
        "    cv.setTrackbarPos('white_lower_3', 'Options', white_lower[2])\n",
        "\n",
        "    cv.setTrackbarPos('white_upper_1', 'Options', white_upper[0]) \n",
        "    cv.setTrackbarPos('white_upper_2', 'Options', white_upper[1])\n",
        "    cv.setTrackbarPos('white_upper_3', 'Options', white_upper[2])\n",
        "\n",
        "    cv.setTrackbarPos('yellow_lower_1', 'Options', yellow_lower[0])\n",
        "    cv.setTrackbarPos('yellow_lower_2', 'Options', yellow_lower[1])\n",
        "    cv.setTrackbarPos('yellow_lower_3', 'Options', yellow_lower[2])\n",
        "\n",
        "    cv.setTrackbarPos('yellow_upper_1', 'Options', yellow_upper[0])\n",
        "    cv.setTrackbarPos('yellow_upper_2', 'Options', yellow_upper[1])\n",
        "    cv.setTrackbarPos('yellow_upper_3', 'Options', yellow_upper[2])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Main\n",
        "\n",
        "Folgender Abschnitt zeigt den gesamten Ablauf der Fahrspurerkennung.\n",
        "\n",
        "In Zeile 2 lässt sich festlegen, in welchem der gegebenen Videos die Spurerkennung durchgeführt werden soll. Zur Auswahl stehen: \"project\", \"challenge\" und \"harder_challenge\".\n",
        "\n",
        "Nach einem Durchlauf des Programms findet sich am Ende des Notebooks die oben angesprochene Tabelle, die den zeitlichen Anteil der einzelnen Funktionen zeigt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Open video file\n",
        "capture = cv.VideoCapture(\"./img/Udacity/\" + video_file + \"_video.mp4\")\n",
        "\n",
        "# Check if camera opened successfully\n",
        "if capture.isOpened() == False:\n",
        "    print(\"Error opening video stream or file\")\n",
        "\n",
        "# variables for frame selection\n",
        "next_frame = True\n",
        "position = 0\n",
        "\n",
        "# Read all frames. This is necessary to be able to use the slider to select a frame\n",
        "frames = []\n",
        "while capture.isOpened():\n",
        "    ret, frame = capture.read()\n",
        "\n",
        "    # Check if there is another frame\n",
        "    if frame is None:\n",
        "        break\n",
        "\n",
        "    frames.append(frame)\n",
        "# Release video capture\n",
        "capture.release()    \n",
        "\n",
        "\n",
        "# define mask ranges\n",
        "yellow_lower = np.array([150, 100, 140])\n",
        "yellow_upper = np.array([255, 140, 200])\n",
        "\n",
        "white_lower = np.array([0, 180, 0])\n",
        "white_upper = np.array([254, 254, 254])\n",
        "\n",
        "if DEBUG_MODE: # enable sliders to change mask ranges in real time \n",
        "    init_sliders()\n",
        "\n",
        "# cache between frames\n",
        "old_left_pts = None\n",
        "old_right_pts = None\n",
        "\n",
        "# reset time analysis\n",
        "time_measurements = {}\n",
        "\n",
        "# Start timer for fps counter\n",
        "start_timer = time.time() - 0.01\n",
        "frame_count = -1\n",
        "\n",
        "# Read every frame\n",
        "while position < len(frames):\n",
        "    frame = frames[position]\n",
        "\n",
        "    # Check if there is another frame\n",
        "    if frame is None:\n",
        "        break\n",
        "\n",
        "    # Calculate Frame rate\n",
        "    if next_frame:\n",
        "\n",
        "        if DEBUG_MODE: # update sliders\n",
        "            white_lower[0] = cv.getTrackbarPos('white_lower_1', 'Options')\n",
        "            white_lower[1] = cv.getTrackbarPos('white_lower_2', 'Options')\n",
        "            white_lower[2] = cv.getTrackbarPos('white_lower_3', 'Options')\n",
        "\n",
        "            white_upper[0] = cv.getTrackbarPos('white_upper_1', 'Options')\n",
        "            white_upper[1] = cv.getTrackbarPos('white_upper_2', 'Options')\n",
        "            white_upper[2] = cv.getTrackbarPos('white_upper_3', 'Options')\n",
        "\n",
        "            yellow_lower[0] = cv.getTrackbarPos('yellow_lower_1', 'Options')\n",
        "            yellow_lower[1] = cv.getTrackbarPos('yellow_lower_2', 'Options')\n",
        "            yellow_lower[2] = cv.getTrackbarPos('yellow_lower_3', 'Options')\n",
        "\n",
        "            yellow_upper[0] = cv.getTrackbarPos('yellow_upper_1', 'Options')\n",
        "            yellow_upper[1] = cv.getTrackbarPos('yellow_upper_2', 'Options')\n",
        "            yellow_upper[2] = cv.getTrackbarPos('yellow_upper_3', 'Options')\n",
        "\n",
        "\n",
        "        frame_count += 1\n",
        "        elapsed_time = time.time() - start_timer\n",
        "        frame_rate = frame_count / elapsed_time\n",
        "        \n",
        "        start_time_measurement(\"frame\")\n",
        "        \n",
        "        # ----------- Preprocessing ---------------\n",
        "        start_time_measurement(\"Preprocessing\")\n",
        "        frame = undistort_image_remap(frame)\n",
        "        frame_undistorted = frame.copy()\n",
        "        frame = warp_image_udacity(frame)\n",
        "        end_time_measurement(\"Preprocessing\")\n",
        "\n",
        "\n",
        "        # ----------- Split in quadrants ---------------\n",
        "        height_cutoff = frame.shape[0] // 2\n",
        "        width_cutoff = frame.shape[1] // 2\n",
        "\n",
        "        left_side = frame[:, :width_cutoff]\n",
        "        right_side = frame[:, width_cutoff:]\n",
        "\n",
        "        l1 = left_side[:height_cutoff, :]\n",
        "        l2 = left_side[height_cutoff:, :]\n",
        "\n",
        "        r1 = right_side[:height_cutoff, :]\n",
        "        r2 = right_side[height_cutoff:, :]\n",
        "\n",
        "        if DEBUG_MODE:\n",
        "            cv.imshow(\"l1.jpg\", l1)\n",
        "            cv.imshow(\"l2.jpg\", l2)\n",
        "            cv.imshow(\"r1.jpg\", r1)\n",
        "            cv.imshow(\"r2.jpg\", r2)\n",
        "        \n",
        "        # ---------  Masking ---------------------qq\n",
        "        start_time_measurement(\"masking\")\n",
        "        l2, brightness = correct_Brightness(l2)\n",
        "        grayscale_frame_l1 = lane_detection(l2, white_lower, white_upper, yellow_lower, yellow_upper)\n",
        "\n",
        "        l1, brightness = correct_Brightness(l1)\n",
        "        grayscale_frame_l2 = lane_detection(l1, white_lower, white_upper, yellow_lower, yellow_upper)\n",
        "\n",
        "        r2, brightness = correct_Brightness(r2)\n",
        "        grayscale_frame_r1 = lane_detection(r2, white_lower, white_upper, yellow_lower, yellow_upper)\n",
        "\n",
        "        r1, brightness = correct_Brightness(r1)\n",
        "        grayscale_frame_r2 = lane_detection(r1, white_lower, white_upper, yellow_lower, yellow_upper)\n",
        "        end_time_measurement(\"masking\")\n",
        "\n",
        "        # Combine the image quadrants\n",
        "        numpy_vertical_l = np.vstack((grayscale_frame_l2, grayscale_frame_l1))\n",
        "        numpy_vertical_r = np.vstack((grayscale_frame_r2, grayscale_frame_r1))\n",
        "        grayscale_frame = np.hstack((numpy_vertical_l, numpy_vertical_r))\n",
        "\n",
        "        if DEBUG_MODE:\n",
        "            cv.imshow(\"Combined, masked frame\", grayscale_frame)\n",
        "        #---------- Sliding Windows ----------\n",
        "        start_time_measurement(\"sliding windows\")\n",
        "        # Convert to grayscale for sliding windows\n",
        "        lefts, rights = sliding_windows(grayscale_frame, minimum_whites=margin)\n",
        "        end_time_measurement(\"sliding windows\")\n",
        "\n",
        "        #---------- Calculate polynomial ----------\n",
        "        start_time_measurement(\"polynomial calculation\")\n",
        "        left_pts, right_pts = calculate_polynomial_points(lefts, rights)\n",
        "        end_time_measurement(\"polynomial calculation\")\n",
        "\n",
        "        # -------- draw plane from all sliding windows ------------\n",
        "        start_time_measurement(\"draw plane\")\n",
        "        if left_pts is not None:\n",
        "            old_left_pts = left_pts        \n",
        "        if right_pts is not None:\n",
        "            old_right_pts = right_pts\n",
        "        drawRecOnFrame(frame_undistorted, old_left_pts[0], old_right_pts[0])\n",
        "        if old_left_pts is  None and old_right_pts is  None:\n",
        "            print(\"No plane drawn\")\n",
        "        end_time_measurement(\"draw plane\")\n",
        "\n",
        "        # -------- Add frame rate to video ------------\n",
        "        if not DEBUG_MODE:\n",
        "            cv.putText(frame_undistorted, \"FPS: \" + str(round(frame_rate)), (0, 25), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv.LINE_AA,)\n",
        "        cv.putText(frame_undistorted, \"Frame: \" + str(frame_count), (0, 50), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv.LINE_AA,)\n",
        "        cv.putText(frame_undistorted, \"Brightness: \" + str(np.round(brightness)), (0, 75), cv.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2, cv.LINE_AA,)\n",
        "        cv.imshow(\"Frame\", frame_undistorted)\n",
        "\n",
        "        end_time_measurement(\"frame\")\n",
        "        next_frame = False\n",
        "    \n",
        "    \n",
        "    pressedKey = cv.waitKey(1) & 0xFF\n",
        "    if pressedKey == ord('q'): # press 'q' to exit the video\n",
        "        break\n",
        "    \n",
        "    if not DEBUG_MODE:\n",
        "        position += 1\n",
        "        next_frame = True\n",
        "    else: # DEBUG_MODE enables the user to navigate through the video with the keyboard ('w', 's') or the slider in the window\n",
        "        if pressedKey == ord('w'):\n",
        "            position += 1\n",
        "            cv.setTrackbarPos('Frame', 'Options', position)\n",
        "            next_frame = True\n",
        "        elif pressedKey == ord('s') and position > 0:\n",
        "            position -= 1\n",
        "            cv.setTrackbarPos('Frame', 'Options', position)\n",
        "            next_frame = True\n",
        "        elif cv.getTrackbarPos('Frame', 'Options') != position:\n",
        "            position = cv.getTrackbarPos('Frame', 'Options')\n",
        "            next_frame = True\n",
        "\n",
        "# When everything done, close all windows\n",
        "cv.destroyAllWindows()\n",
        "\n",
        "# create time measurement analysis and print out results\n",
        "analyse_time_measurements()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.11.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
